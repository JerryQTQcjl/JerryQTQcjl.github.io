{"meta":{"title":"Jerry Chan","subtitle":"","description":"","author":"Jerry Chan","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2023-03-25T16:16:19.000Z","updated":"2023-03-25T16:17:15.844Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-03-25T16:15:37.000Z","updated":"2023-03-25T16:16:54.923Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"k8s 搭建","slug":"k8s-搭建","date":"2023-03-31T15:45:37.000Z","updated":"2023-03-31T15:46:40.276Z","comments":true,"path":"2023/03/31/k8s-搭建/","link":"","permalink":"http://example.com/2023/03/31/k8s-%E6%90%AD%E5%BB%BA/","excerpt":"","text":"k8s搭建说明本文系搭建 kubernetes v1.21.3 版本集群笔记，使用三台虚拟机作为 CentOS7.9 系统测试机，安装 kubeadm、kubelet、kubectl 均使用 yum 安装，网络组件选用的是 flannel。 环境准备部署集群没有特殊说明均使用 root 用户执行命令。 2.1 硬件信息 ip hostname mem disk explain 192.168.85.2 192.168.85.2 2G 40GB k8s 控制平面节点 192.168.85.3 192.168.85.3 2G 40GB k8s 执行节点1 192.168.85.4 192.168.85.4 2G 40GB k8s 执行节点2 2.2 软件信息 12345software versionCentOS CentOS Linux release 7.9.2009 (Core)Kubernetes 1.21.3Docker 20.10.8Kernel 5.4.138-1.el7.elrepo.x86_64 2.3 禁用 swapswap 仅当内存不够时会使用硬盘块充当额外内存，硬盘的 io 较内存差距极大，禁用 swap 以提高性能各节点均需执行： 123swapoff -a cp /etc/fstab /etc/fstab.bakcat /etc/fstab.bak | grep -v swap &gt; /etc/fstab 2.4 关闭 SELinux 关闭 SELinux，否则 kubelet 挂载目录时可能报错 Permission denied，可以设置为 permissive 或 disabled，permissive 会提示 warn 信息各节点均需执行： 12setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 2.5 设置时区、同步时间 12timedatectl set-timezone Asia/Shanghai systemctl enable --now chronyd 查看同步状态： 1timedatectl status 2.6 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld 安装必要依赖1yum install -y yum-utils device-mapper-persistent-data lvm2 3.1 添加 aliyun docker-ce yum 源 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 重建 yum 缓存 1yum makecache fast 3.2 安装 Docker 查看可用 docker 版本 1yum list docker-ce.x86_64 --showduplicates | sort -r ==安装指定版本 Docker== yum install -y docker-ce-20.10.14-3.el7 这里以安装 20.10.14 版本举例，注意版本号不包含 : 与之前的数字。 3.3 确保网络模块开机自动加载 12lsmod | grep overlay lsmod | grep br_netfilter 若上面命令无返回值输出或提示文件不存在，需执行以下命令： 123456cat &gt; /etc/modules-load.d/docker.conf &lt;&lt;EOF overlay br_netfilter EOFmodprobe overlay modprobe br_netfilter 3.4 使桥接流量对 iptables 可见各个节点均需执行： 12345cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 验证是否生效，均返回 1 即正确。 12sysctl -n net.bridge.bridge-nf-call-iptables sysctl -n net.bridge.bridge-nf-call-ip6tables 3.5 配置 Docker 1mkdir /etc/docker 修改 cgroup 驱动为 systemd [k8s官方推荐]、限制容器日志量、修改存储类型，最后的 docker 家目录可修改： 123456789101112131415cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"registry-mirrors\": [\"https://gp8745ui.mirror.aliyuncs.com\"], \"data-root\": \"/data/docker\"&#125;EOF 服务脚本第 13 行修改： 123vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --default-ulimit core=0:0systemctl daemon-reload 添加开机自启，立即启动： 1systemctl enable --now docker 3.6 验证 Docker 是否正常 查看docker信息，判断是否与配置一致 部署 Kubernetes 集群如未说明，各节点均需执行如下步骤： 4.1 添加 kubernetes 源 123456789cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 重建yum缓存，输入y添加证书认证 1yum makecache fast 4.2 安装 kubeadm、kubelet、kubectl各节点均需安装 kubeadm、kubelet 12345yum list docker-ce.x86_64 --showduplicates | sort -rversion=1.21.3-0yum install -y kubelet-v1.21.0 kubeadm-v1.21.0 kubectl-v1.21.0systemctl enable kubelet --now 4.3 配置自动补全命令 安装 bash 自动补全插件 1yum install bash-completion -y 设置 kubectl 与 kubeadm 命令补全，下次 login 生效 12kubectl completion bash &gt;/etc/bash_completion.d/kubectlkubeadm completion bash &gt; /etc/bash_completion.d/kubeadm 4.4 为 Docker 设定使用的代理服务(暂跳过该步骤，由阿里云镜像解决)Kubeadm 部署 Kubernetes 集群的过程中，默认使用 Google 的 Registry 服务 k8s.gcr.io 上的镜像，例如k8s.grc.io/kube-apiserver 等，但国内无法访问到该服务。必要时，可自行设置合适的代理来获取相关镜像，或者从 Dockerhub 上下载镜像至本地后自行对镜像打标签。 这里简单说明一下设置代理服务的方法。编辑 /lib/systemd/system/docker.service 文件，在 [Service] 配置段中添加类似如下内容，其中的 PROXY_SERVER_IP 和 PROXY_PORT 要按照实际情况修改。 123Environment=\"HTTP_PROXY=http://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"HTTPS_PROXY=https://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"NO_PROXY=192.168.4.0/24\" 配置完成后需要重载 systemd，并重新启动 docker 服务： 12systemctl daemon-reloadsystemctl restart docker.service 需要特别说明的是，由 kubeadm 部署的 Kubernetes 集群上，集群核心组件 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd 等均会以静态 Pod 的形式运行，它们所依赖的镜像文件默认来自于 k8s.gcr.io 这一 Registry 服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种，本示例将选择使用更易于使用的前一种方式： 使用能够到达该服务的代理服务使用国内的镜像服务器上的服务，例如 gcr.azk8s.cn/google_containers 和 registry.aliyuncs.com/google_containers 等（经测试，v1.22.0 版本已停用）4.5 查看指定 k8s 版本需要哪些镜像 123456789kubeadm config images list --kubernetes-version v1.21.0k8s.gcr.io/kube-apiserver:v1.21.0k8s.gcr.io/kube-controller-manager:v1.21.0k8s.gcr.io/kube-scheduler:v1.21.0k8s.gcr.io/kube-proxy:v1.21.0k8s.gcr.io/pause:3.4.1k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.0 4.6 拉取镜像 1vim pullimages.sh 1234567891011121314151617181920212223#!/bin/bash# pull images ver=v1.21.0registry=registry.cn-hangzhou.aliyuncs.com/google_containersimages=`kubeadm config images list --kubernetes-version=$ver |awk -F '/' '&#123;print $2&#125;'` for image in $imagesdoif [ $image != coredns ];then docker pull $&#123;registry&#125;/$image if [ $? -eq 0 ];then docker tag $&#123;registry&#125;/$image k8s.gcr.io/$image docker rmi $&#123;registry&#125;/$image else echo \"ERROR: 下载镜像报错，$image\" fielse docker pull coredns/coredns:1.8.0 docker tag coredns/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0 docker rmi coredns/coredns:1.8.0fidone 4.7 修改 kubelet 配置默认 cgroup driver 123456mkdir /var/lib/kubeletcat &gt; /var/lib/kubelet/config.yaml &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemdEOF 4.8 初始化 master 节点仅 192.168.85.2 节点需要执行此步骤。 4.8.1 生成 kubeadm 初始化配置文件[可选] 仅当需自定义初始化配置时用。 1kubeadm config print init-defaults &gt; kubeadm-config.yaml 修改配置文件： 12localAPIEndpoint: advertiseAddress: 1.2.3.4 替换为： 1234localAPIEndpoint: advertiseAddress: 192.168.85.2 name: 192.168.85.2kubernetesVersion: 1.21.0 123networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 替换为： 1234kubernetesVersion: 1.21.0networking: podSubnet: \"10.244.0.0/16\" serviceSubnet: 10.96.0.0/12 4.8.2 测试环境是否正常 1kubeadm init phase preflight 4.8.3 初始化 master10.244.0.0/16 是 flannel 固定使用的 IP 段，设置取决于网络组件要求。 1kubeadm init --config=kubeadm-config.yaml --ignore-preflight-errors=2 --upload-certs | tee kubeadm-init.log 4.8.4 为日常使用集群的用户添加 kubectl 使用权限 123456su - iuskyemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/admin.confsudo chown $(id -u):$(id -g) $HOME/.kube/admin.confecho \"export KUBECONFIG=$HOME/.kube/admin.conf\" &gt;&gt; ~/.bashrcexit 4.8.5 配置 master 认证 12echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' &gt;&gt; /etc/profile . /etc/profile 如果不配置这个，会提示如下输出：The connection to the server localhost:8080 was refused - did you specify the right host or port?此时 master 节点已经初始化成功，但是还未安装网络组件，还无法与其他节点通讯。 4.8.6 安装网络组件以 flannel 为例： 1234curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull quay.io/coreos/flannel:v0.14.0kubectl apply -f kube-flannel.yml 4.8.7 查看 192.168.85.2 节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 如果 STATUS 提示 NotReady，可以通过 kubectl describe node 192.168.85.2 查看具体的描述信息，性能差的服务器到达 Ready 状态时间会长些。 4.9 初始化 node 节点并加入集群4.9.1 获取加入 kubernetes 的命令访问 192.168.85.2 输入创建新 token 命令： 1kubeadm token create --print-join-command 同时输出加入集群的命令： 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 这个 token 也可以使用上述 master 上执行的初始化输出结果。 4.9.2 在 node 节点上执行加入集群的命令 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 4.10 查看集群节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 4.11 部署 Dashboard4.11.1 部署 1curl -o recommended.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 默认 Dashboard 只能集群内部访问，修改 Service 为 NodePort 类型，暴露到外部： 12345678910111213141516vi recommended.yamlkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: ports: - port: 443 targetPort: 8443 nodePort: 30001 type: NodePort selector: k8s-app: kubernetes-dashboard 12345678910111213kubectl apply -f recommended.yaml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull kubernetesui/dashboard:v2.3.1docker pull kubernetesui/metrics-scraper:v1.0.6kubectl apply -f recommended.yamlkubectl get pods,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 0/1 ContainerCreating 0 52spod/kubernetes-dashboard-67484c44f6-shtz7 0/1 ContainerCreating 0 52sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 52sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 53s 查看状态正在创建容器中，稍后再次查看： 123456NAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 1/1 Running 0 2m11spod/kubernetes-dashboard-67484c44f6-shtz7 1/1 Running 0 2m11sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 2m11sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 2m12s 访问地址：https://NodeIP:30001；使用 Firefox 浏览器，Chrome 浏览器打不开不信任 SSL 证书的网站。 创建 service account 并绑定默认 cluster-admin 管理员集群角色： 123kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/&#123;print $1&#125;') 这里需要注意粘贴的时候有可能被换行，如果被换行，可在记事本中设置为一行。 使用输出的 token 登录 Dashboard。 问题解决1、dial tcp 10.96.0.1:443: connect: no route to host 123456systemctl stop dockersystemctl stop kubeletiptables --flushiptables -tnat --flushsystemctl start kubeletsystemctl start docker 2、failed to delegate add: failed to set bridge addr: “cni0“ already has an IP address different from 1 1https://blog.csdn.net/weixin_42562106/article/details/123749291 3、failed to add vxlanRoute (10.244.0.0/24 -&gt; 10.244.0.0): network is down 1234ip link delete flannel.1systemctl restart networkkubectl delete -f kube-flannel.ymlkubectl apply -f kube-flannel.yml 4、Get http://10.244.0.3:8181/ready: dial tcp 10.244.0.3:8181: connect: connection refused 12#重新corednskubectl -n kube-system rollout restart deployment/coredns 以上几个问题遇到可以先尝试重启所有的机器，如果不行在通过上述方案解决 参考：https://www.iuskye.com/2021/08/10/k8s-kubeadm-1213.html","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"redisson3.15.2 公平锁任务丢失","slug":"redisson3.15.2 公平锁任务丢失","date":"2023-03-31T15:22:45.000Z","updated":"2023-03-31T15:37:33.829Z","comments":true,"path":"2023/03/31/redisson3.15.2 公平锁任务丢失/","link":"","permalink":"http://example.com/2023/03/31/redisson3.15.2%20%E5%85%AC%E5%B9%B3%E9%94%81%E4%BB%BB%E5%8A%A1%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"场景： 线索流转改成 redisson 公平锁，任务耗时长，导致部分任务丢失。 一、redisson 公平锁实现1、redis 中的 K/V12345678list: redisson_lock_queue:&#123;test&#125; elem: UUID:threadIdzset: redisson_lock_timeout:&#123;key&#125; elem: UUID:threadId score: timeout = ttl + 线程等待时间(5*60000ms) + 当前时间戳hset: key hashKey: UUID:threadId hashVal: 1 2、上锁流程（这里就不放源码了，感兴趣可以自己看看） org.redisson.RedissonFairLock#tryLockInnerAsync 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 如果当前没有线程占用锁，则上锁 (ttl = watchdogtimeout)，同时redisson_lock_timeout:{key} 中所有的 score - 线程等待时间(5*60000ms) 如果存在锁，且是被当前线程占用的，则 hashVal 加一 如果存在锁，且不是被当前线程占用的，同时已经加入过等待队列，则返回当前线程在队列中的 ttl 如果存在锁，且不是被当前线程占用的，并且未加入等待队列，则加入等待队列，timeout= ttl + 线程等待时间(5*60000ms) + 当前时间戳，ttl 为队列中最后一个元素的 timeout - current 或者 锁的超时时间 3、订阅 redisson_lock__channel:{fairLock}:UUID:threadId此处阻塞线程，直到消息队列中有消息发送为止 4、解锁流程 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 判断锁是否存在，如果不存在则说明锁已经被释放了，判断等待队列中是否有elem，有的话取出第一个 publish message 如果锁存在，且非本线程持有，则直接返回 null 如果所存在，且是当前线程持有，并且获取所次数大于 1，则 hashVal减一，更新锁超时时间 如果所存在，且是当前线程持有，释放锁，判断等待队列中是否有elem，有的话取出第一个 publish message 5、获取到锁之后，取消订阅 redisson_lock__channel:{fairLock}:UUID:threadId 二、问题分析1、由于测试环境第三方鉴权接口较慢，每分配一条线索需要3-4s 2、存量线索有 1500+ 条，每 200 条作为一个任务，总共拆分 8 个任务，每个任务需要执行 200*3 = 600+ s 3、redisson 公平锁线程等待时间 (5*60000ms)，也就是说单个任务执行完至少会有一个任务过期，在 unlock 或者 lock 操作是会先清除过期任务 4、由于获取不到锁，线程会订阅 redisson_lock__channel:{fairLock}:UUID:threadId，阻塞知道接收到消息，又由于等待队列里的线程被清了，这个消息队列永远不会收到消息，所以线程一直阻塞，且任务无法执行，资源被占用。 5、代码模拟： 1234567891011121314151617public static void main(String[] args) &#123; Config config = new Config(); config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\"); Redisson redissonClient = (Redisson) Redisson.create(config); IntStream.range(0, 10).forEach(i -&gt; &#123; new Thread(() -&gt; &#123; //RedissonFairLock lock = (RedissonFairLock) redissonClient.getFairLock(\"test\"); RedissonFairLock lock = new RedissonFairLock(redissonClient.getCommandExecutor(), \"test\", 2000); lock.lock(); //最终输出次数小于10 System.out.println(i); ThreadUtil.sleep(10000); lock.unlock(); &#125;).start(); &#125;); &#125; 参考: Redisson分布式锁之公平锁原理","categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"apollo 集群架构","slug":"apollo-集群架构","date":"2023-03-25T18:25:23.000Z","updated":"2023-03-31T15:44:48.277Z","comments":true,"path":"2023/03/26/apollo-集群架构/","link":"","permalink":"http://example.com/2023/03/26/apollo-%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/","excerpt":"","text":"apollo 集群架构apollo 组成结构apollo 集群主要有三个部分组成，config-service，admin-service，portal config-service: 主要为应用客户端提供服务，包括配置的读取、推送等功能，config-service 内置 eureka，已提供 admin-service, portal 的服务发现、服务注册 admin-service: 主要为 apollo-portal 提供服务，包括应用配置管理、发布等功能 portal: 是 apollo 提供的服务配置前端页面 apollo 部署方案这个官网提供的高可用双环境架构，更多的架构方案可参考 docker-compose 部署 apollo 集群 执行 sql 脚本，生成 apollo 需要的 db 表 编写 docker-compose 文件，本次只搭建开发环境模拟单机单环境，config-service，admin-service，portal 都各自部署一个容器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: '2'networks: net:services: #开发环境configService，eureka apollo-configservice-dev: image: apolloconfig/apollo-configservice container_name: configservice-dev ports: - 8080:8080 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - eureka.instance.ip-address=192.168.0.103 networks: - net apollo-adminservice-dev: image: apolloconfig/apollo-adminservice container_name: adminservice-dev ports: - 8090:8090 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 #这里需要配置开环环境的configService - eureka.service.url=http://configservice-dev:8080/eureka/ depends_on: - apollo-configservice-dev networks: - net apollo-portal: image: apolloconfig/apollo-portal container_name: apollo-portal ports: - 8070:8070 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloPortalDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - spring.profiles.active=auth - APOLLO_PORTAL_ENVS=dev,fat,pre,gray,pro - DEV_META=http://configservice-dev:8080 # 以下这些暂时使用开发环境的 - FAT_META=http://configservice-dev:8080 - PRE_META=http://configservice-dev:8080 - GRAY_META=http://configservice-dev:8080 - PRO_META=http://configservice-dev:8080 depends_on: - apollo-adminservice-dev networks: - net","categories":[],"tags":[{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"consul 集群搭建及注意事项","slug":"consul-集群搭建及注意事项","date":"2023-03-25T17:16:50.000Z","updated":"2023-03-31T15:45:02.909Z","comments":true,"path":"2023/03/26/consul-集群搭建及注意事项/","link":"","permalink":"http://example.com/2023/03/26/consul-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","excerpt":"","text":"集群搭建本次利用 docker 搭建 consul 集群，利用 docker-compose 统一管理 集群包含三个 server agent: node1、node2、node3 集群包含两个 client agent: node4、node5，client agent 提供 ui **1、下载 docker 镜像 ** 1docker pull docker 2、编辑 docker-compose.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253version: '2'networks: byfn:services: consul1: image: consul container_name: node1 command: agent -server -bootstrap-expect=3 -node=node1 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 networks: - byfn consul2: image: consul container_name: node2 command: agent -server -retry-join=node1 -node=node2 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul3: image: consul container_name: node3 command: agent -server -retry-join=node1 -node=node3 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul4: image: consul container_name: node4 command: agent -retry-join=node1 -node=ndoe4 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8500:8500 depends_on: - consul2 - consul3 networks: - byfn consul5: image: consul container_name: node5 command: agent -retry-join=node1 -node=ndoe5 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8501:8500 depends_on: - consul2 - consul3 networks: - byfn 命令行参数-server：表示当前使用的 server 模式；如果没有指定，则表示是 client 模式。 -node：指定当前节点在集群中的名称。 -config-dir：指定配置文件路径，定义服务的；路径下面的所有 .json 结尾的文件都被访问；缺省值为：/consul/config。 -data-dir： consul 存储数据的目录；缺省值为：/consul/data。 -datacenter：数据中心名称，缺省值为 dc1。 -ui：使用 consul 自带的 web UI 界面 。 -join：加入到已有的集群中。 -retry-join：与 -join 类似，但允许重试连接，直到连接成功。一旦成功加入成员列表中的成员，它将永远不会尝试再次加入。然后，代理商将仅通过八卦维持其会员资格。允许配置多个 -retry-join，然后节点会按照顺序加入和重试，直到第一个成功。 -enable-script-checks： 检查服务是否处于活动状态，类似开启心跳。 -advertise：通告地址用于更改我们向集群中其他节点通告的地址。相当于 -bind 不可用时更改 -bind 地址，可以理解为 -bind 的一个备选方案。 -bind： 绑定服务器的 ip 地址，缺省值：“0.0.0.0”，这意味着 consul 将绑定到本地机器上的所有地址，并将私有 IPv4 地址通告给集群的其余节点，如果有多个私有 IPv4 地址可用，consul 将在启动时退出并报错。consul 1.1.0 之后，可以结合 go-sockaddr template使用，例如 1$ consul agent -bind &#39;&#123;&#123; GetPrivateInterfaces | include &quot;network&quot; &quot;10.0.0.0&#x2F;8&quot; | attr &quot;address&quot; &#125;&#125;&#39; -client：客户端可访问 ip，缺省值为：“127.0.0.1”，即仅允许环回连接。 -bootstrap-expect：在一个 datacenter 中期望的 server 节点数目，consul 启动时会一直等待直到达到这个数目的server才会引导整个集群。这个参数的值在同一个 datacenter 的所有server节点上必须保持一致。 -bootstrap：用来控制一个 server 是否运行在 bootstrap 模式：当一个 server 处于 bootstrap 模式时，它可以选举自己为 leader；注意在一个 datacenter 中只能有一个 server 处于 bootstrap 模式。所以这个参数一般只能用在只有一个 server 的开发环境中，在有多个 server 的 cluster 产品环境中，不能使用这个参数，否则如果多个 server 都标记自己为 leader 那么会导致数据不一致。另外该标记不能和 -bootstrap-expect 同时指定。 服务注册1、通过 config-file 注册服务，编辑 xxx.json，然后放在 /consul/config 目录下（默认情况）,当然也可以通过 consul 命令行加载配置。 12345678910111213141516171819202122&#123; \"services\": [ &#123; \"id\": \"hertz-demo-001\", \"name\": \"hertz-demo\", \"tags\": [ ], \"address\": \"192.168.0.103\", \"port\": 5000, \"checks\": [ &#123; \"http\": \"http://192.168.0.103:5000/ping\", \"tlsSkipVerify\": false, \"method\": \"GET\", \"interval\": \"10s\", \"timeout\": \"1s\", \"deregisterCriticalServiceAfter\": \"30s\" &#125; ] &#125; ] &#125; 2、执行 consul 命令重载配置文件 1consul reload 3、启动 web 服务，这里使用自己熟悉的语言写一个简单的服务即可，这里用的是 go http 框架 hertz: 1234567891011121314151617181920212223package mainimport ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" \"log\")func main() &#123; s := server.Default( server.WithHostPorts(\"192.168.0.103:5000\"), ) s.GET(\"/ping\", func(ctx context.Context, req *app.RequestContext) &#123; log.Print(req.ClientIP(), \" ping\") req.JSON(consts.StatusOK, utils.H&#123;\"ping\": \"pong\"&#125;) &#125;) s.Spin()&#125; 4、查看服务健康信息，passing 参数表示是否过滤不健康的服务 1curl http://127.0.0.1:8500/v1/health/service/hertz-demo\\?passing\\=true 5、注销服务 1curl --request PUT 127.0.0.1:8501/v1/agent/service/deregister/hertz-demo-001 6、consul ui 界面上显示如下： 注意事项1、注册服务时，check 参数 method GET 注意 GET 需要大写，否则健康检查会失败 2、服务注册的 client agent 挂了，那么 consul 会认为服务也挂了，并不会做故障转移，也不会同步原本 client agent 下的服务信息 3、服务注册需要确保网络能联通 扩展1、官方推荐的架构方式 官方建议一个集群部署 3-5 个 server agent，每个服务的服务器部署一个 client agent，如下如所示： 2、如是想要统一管理 consul agent，那可以参考另一种架构方式： 参考：https://mp.weixin.qq.com/s/ecmqqWuMho2a0xhaF1vFNAhttps://www.cnblogs.com/brady-wang/p/14440649.html","categories":[],"tags":[{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-03-25T16:04:15.447Z","updated":"2023-03-25T16:04:15.447Z","comments":true,"path":"2023/03/26/hello-world/","link":"","permalink":"http://example.com/2023/03/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"},{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"}]}