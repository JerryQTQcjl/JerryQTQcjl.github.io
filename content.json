{"meta":{"title":"Jerry Chan","subtitle":"","description":"","author":"Jerry Chan","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2023-03-25T16:16:19.000Z","updated":"2023-03-25T16:17:15.844Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-03-25T16:15:37.000Z","updated":"2023-03-25T16:16:54.923Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"shardingjdbc 数据分片笔记","slug":"shardingjdbc-数据分片笔记","date":"2024-06-29T13:12:00.000Z","updated":"2024-06-29T13:34:31.709Z","comments":true,"path":"2024/06/29/shardingjdbc-数据分片笔记/","link":"","permalink":"http://example.com/2024/06/29/shardingjdbc-%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87%E7%AC%94%E8%AE%B0/","excerpt":"","text":"分片路由引擎org.apache.shardingsphere.core.route.type.RoutingEngine 具体选择什么分片路由引擎取决于执行的 sql org.apache.shardingsphere.core.route.router.sharding.ShardingRouter#route 分片路由策略org.apache.shardingsphere.core.strategy.route.ShardingStrategy org.apache.shardingsphere.core.strategy.route.none.NoneShardingStrategy 不进行分片路由 org.apache.shardingsphere.core.strategy.route.none.NoneShardingStrategy 根据行表达式路由，不支持按条件范围分片 org.apache.shardingsphere.core.strategy.route.standard.StandardShardingStrategy 经典分片策略，内置按值分片算法，按范围分配算法 org.apache.shardingsphere.core.strategy.route.complex.ComplexShardingStrategy 组合条件分片策略，逻辑需要执行实现，==同时如果某些逻辑 StandardShardingStrategy 无法实现也可以考虑使用 ComplexShardingStrategy==。 org.apache.shardingsphere.core.strategy.route.hint.HintShardingStrategy 强制分片策略，适合需要按照分数据库的列进行分片的场景，比如根据 来源ip 进行分片，底层基于 ThreadLocal 实现，需要在执行 sql 前指定分片值。 使用 DEMO1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package org.apache.shardingsphere.example.sharding.raw.jdbc.config;import org.apache.shardingsphere.api.config.sharding.KeyGeneratorConfiguration;import org.apache.shardingsphere.api.config.sharding.ShardingRuleConfiguration;import org.apache.shardingsphere.api.config.sharding.TableRuleConfiguration;import org.apache.shardingsphere.api.config.sharding.strategy.ComplexShardingStrategyConfiguration;import org.apache.shardingsphere.api.config.sharding.strategy.StandardShardingStrategyConfiguration;import org.apache.shardingsphere.api.sharding.complex.ComplexKeysShardingAlgorithm;import org.apache.shardingsphere.api.sharding.complex.ComplexKeysShardingValue;import org.apache.shardingsphere.core.constant.properties.ShardingPropertiesConstant;import org.apache.shardingsphere.example.algorithm.PreciseModuloShardingTableAlgorithm;import org.apache.shardingsphere.example.config.ExampleConfiguration;import org.apache.shardingsphere.example.core.api.DataSourceUtil;import org.apache.shardingsphere.shardingjdbc.api.ShardingDataSourceFactory;import javax.sql.DataSource;import java.sql.SQLException;import java.util.*;public final class ShardingMultiDatabasesAndTablesConfigurationPrecise implements ExampleConfiguration &#123; @Override public DataSource getDataSource() throws SQLException &#123; ShardingRuleConfiguration shardingRuleConfig = new ShardingRuleConfiguration(); shardingRuleConfig.getTableRuleConfigs().add(getOrderTableRuleConfiguration()); shardingRuleConfig.getTableRuleConfigs().add(getOrderItemTableRuleConfiguration()); shardingRuleConfig.getBindingTableGroups().add(\"t_order, t_order_item\"); shardingRuleConfig.getBroadcastTables().add(\"t_address\");// shardingRuleConfig.setDefaultDatabaseShardingStrategyConfig(new InlineShardingStrategyConfiguration(\"user_id\", \"demo_ds_$&#123;user_id % 2&#125;\")); shardingRuleConfig.setDefaultDatabaseShardingStrategyConfig(new ComplexShardingStrategyConfiguration(\"user_id\", new CustomComplexKeysShardingAlgorithm())); shardingRuleConfig.setDefaultTableShardingStrategyConfig(new StandardShardingStrategyConfiguration(\"order_id\", new PreciseModuloShardingTableAlgorithm())); Properties props = new Properties(); props.put(ShardingPropertiesConstant.SQL_SHOW.getKey(), true); return ShardingDataSourceFactory.createDataSource(createDataSourceMap(), shardingRuleConfig, props); &#125; private static TableRuleConfiguration getOrderTableRuleConfiguration() &#123; TableRuleConfiguration result = new TableRuleConfiguration(\"t_order\", \"demo_ds_$&#123;0..1&#125;.t_order_$&#123;[0, 1]&#125;\"); result.setKeyGeneratorConfig(new KeyGeneratorConfiguration(\"SNOWFLAKE\", \"order_id\", getProperties())); return result; &#125; private static TableRuleConfiguration getOrderItemTableRuleConfiguration() &#123; TableRuleConfiguration result = new TableRuleConfiguration(\"t_order_item\", \"demo_ds_$&#123;0..1&#125;.t_order_item_$&#123;[0, 1]&#125;\"); result.setKeyGeneratorConfig(new KeyGeneratorConfiguration(\"SNOWFLAKE\", \"order_item_id\", getProperties())); return result; &#125; private static Map&lt;String, DataSource&gt; createDataSourceMap() &#123; Map&lt;String, DataSource&gt; result = new HashMap&lt;&gt;(); result.put(\"demo_ds_0\", DataSourceUtil.createDataSource(\"demo_ds_0\")); result.put(\"demo_ds_1\", DataSourceUtil.createDataSource(\"demo_ds_1\")); return result; &#125; private static Properties getProperties() &#123; Properties result = new Properties(); result.setProperty(\"worker.id\", \"123\"); return result; &#125; public static class CustomComplexKeysShardingAlgorithm implements ComplexKeysShardingAlgorithm&lt;Integer&gt; &#123; @Override public Collection&lt;String&gt; doSharding(Collection&lt;String&gt; availableTargetNames, ComplexKeysShardingValue&lt;Integer&gt; shardingValue) &#123; List&lt;String&gt; result = new ArrayList&lt;&gt;(); shardingValue.getColumnNameAndShardingValuesMap().forEach((column, valueList) -&gt; &#123; if (Objects.equals(column, \"user_id\")) &#123; valueList.forEach(value -&gt; &#123; if (value % 2 == 1) &#123; result.addAll(availableTargetNames); &#125; else &#123; Optional&lt;String&gt; optional = availableTargetNames.stream().filter(name -&gt; name.endsWith(String.valueOf(value % 2))).findFirst(); optional.ifPresent(result::add); &#125; &#125;); &#125; &#125;); return result; &#125; &#125;&#125; 注意事项 ==如果 sql 语句没有命中分片条件，将在所有的物理表中执行 sql== hint 必须要在执行 sql 前设置条件值，否则会和上述一样因未命中分片条件而在所有物理表执行 sql 目前支持原生的分片策略，如果要支持自定义的可能得把大部分上游组件给改了 支持很多 sqi 扩展钩子，例如 org.apache.shardingsphere.sql.parser.hook.SPIParsingHook","categories":[],"tags":[]},{"title":"记录一次增量数据一致性问题","slug":"记录一次增量数据一致性问题","date":"2023-11-19T02:11:45.000Z","updated":"2023-11-19T15:11:48.859Z","comments":true,"path":"2023/11/19/记录一次增量数据一致性问题/","link":"","permalink":"http://example.com/2023/11/19/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","excerpt":"","text":"最近收到客户投诉出现数据无法操作的问题，经过排查，Mysql 和 ES 数据出现一致性问题。我们的使用场景是定于 Mysql binlog 投递到 Kafka，增量服务顺序消费 Kafka 中的消息，更新 ES 中的文档。 索引结构在分析问题前先简单看下索引结构，这里只看出现问题的字段，主要就是用于存储标签 id 的一个 long 类型的字段，存的是一个数组。 12345678910&#123; \"mappings\" : &#123; \"f_tag_id\" : &#123; \"type\" : \"long\" &#125;, \"f_status\" : &#123; \"type\" : \"byte\" &#125; &#125;&#125; 问题分析 先简单看一下标签增量的逻辑，由于标签存的是一个数组，所以在更新的时候是先把文档查询出来，再对相应的数组字段做元素的新增或者删除，然后再重新索引到 ES 中。 生产中的场景是同时有两个 binlog 并发，但是是分两次消费（同一批次的消息我们这边会在内存进行组装），A 表的 binlog 先更新 f_status，B 表 (标签关联表) 的 binlog 先将文档查询出来，更新完之后再将整个文档索引回 ES（更新代码通用性，所以是更新整个文档）。 由于 ES 索引更新刷盘并非实时的，导致 B 表查询出来的文档是旧的，所以 B 表在更新索引时将旧的数据覆盖了新的数据。 举一个简单的例子： 假设原本 f_status = 0, f_tag_id = [] A 表更新 f_status =1，此时 f_status = 1, f_tag_id = [] B 表更新 f_tag_id = [1]，由于 ES 刷盘存在延迟，导致更新的数据为 f_status = 0, f_tag_id = [1] 解决方案 最初的想法是只更新 f_tag_id 字段，但是这样其实还是会有问题，举个例子 B 表 (标签关联表) 针对同一条记录两个标签关联数据，意味着会有两个 binlog 生成，假设很不巧，Kafka 消费者分了两个批次去更新索引，就会造成和上述类似的场景。 假设原本 f_tag_id = [1,2] 第一条 binlog 更新 f_tag_id = [1,2,3] 第二条 binlog 消费时，先查询 f_tag_id = [1,2]，此时在去更新 f_tag_id 就会变成 [1,2,4]，还是会出现数据不一致问题 第一个方案行不通，那能不能让 f_tag_id 像更新其他字段一样，不需要先查询出来组装再重新索引回 ES，答案是可以，那就是利用 ES 脚本更新，因为更新操作是可以直接更新内存的数据，所以数据是实时的。 12345678910111213141516171819202122232425&#123; \"script\": &#123; \"source\": \"\"\" List tagIdList = ctx._source.f_tag_id == null ? new ArrayList() : ctx._source.f_tag_id; if(\"insert\".equals(params.method) &amp;&amp; !tagIdList.contains(params.changeTagId))&#123; tagIdList.add(params.changeTagId); &#125; else if(\"delete\".equals(params.method) &amp;&amp; tagIdList.contains(params.changeTagId))&#123; tagIdList.remove(tagIdList.indexOf(params.changeTagId)); &#125; ctx._source.f_tag_id = tagIdList; \"\"\", \"lang\": \"painless\", \"params\": &#123; \"changeTagId\": 12, \"method\": \"insert\" &#125; &#125;, \"upsert\": &#123; \"f_tag_id\": [12] &#125;&#125; 大概思路就是利用脚本直接针对 f_tag_id 进行元素变更，这样就不用担心查出来的 f_tad_id 是旧的了。 参考https://www.elastic.co/guide/en/elasticsearch/reference/7.14/docs-update.htmlhttps://www.elastic.co/guide/en/elasticsearch/painless/7.14/painless-types.html","categories":[],"tags":[{"name":"ES","slug":"ES","permalink":"http://example.com/tags/ES/"}]},{"title":"分布式事务之 2PC 的 XA 规范实现","slug":"分布式事务之-2PC-的-XA-规范实现","date":"2023-07-25T02:11:00.000Z","updated":"2023-07-25T02:16:46.841Z","comments":true,"path":"2023/07/25/分布式事务之-2PC-的-XA-规范实现/","link":"","permalink":"http://example.com/2023/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%B9%8B-2PC-%E7%9A%84-XA-%E8%A7%84%E8%8C%83%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"刚开始学习分布式事务时，第一个解决方案就是 2PC，但是一直没有写过 demo，也没有生产中使用过这种方案，随意是既熟悉又陌生。本文只在实现一个 2PC XA方案的 demo，万一将来需要用到这种方案不至于少也不懂。 2PC 简介2PC 主要分为两个阶段，准备阶段、提交阶段。 准备阶段：TM（事务管理器）向 RM（资源管理器）发送 Prepare 消息，RM 本地执行事务，此时 RM 并未提交本地事务（上锁），RM 发送执行成功/失败给到 TM 提交阶段：TM 向 RM 发送 Commit/Rollback 消息，RM 根据 TM 发送的消息执行提交或回滚本地事务。 以下是网上找的一张 2PC 的流程图： XA 方案 2PC 的传统方案是在数据库层面实现的，如 Oracle、MySQL 都支持 2PC 协议，为了统一标准减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织 Open Group 定义了分布式事务处理模型DTP（Distributed Transaction Processing Reference Model）。 DTP 模型定义如下角色： AP（Application Program）：即应用程序，可以理解为使用 DTP 分布式事务的程序。 RM（Resource Manager）：即资源管理器，可以理解为事务的参与者，一般情况下是指一个数据库实例，通过资源管理器对该数据库进行控制，资源管理器控制着分支事务。 TM（Transaction Manager）：事务管理器，负责协调和管理事务，事务管理器控制着全局事务，管理事务生命周期，并协调各个 RM。全局事务是指分布式事务处理环境中，需要操作多个数据库共同完成一个工作，这个工作即是一个全局事务。 DTP 模型定义TM和RM之间通讯的接口规范叫 XA，简单理解为数据库提供的 2PC 接口协议，基于数据库的 XA 协议来实现 2PC 又称为 XA 方案 XA 执行流程：（作者比较懒，什么都没留下） 流程可以参看大佬的博客：https://zhuanlan.zhihu.com/p/263555694 具体实现： Java事务API（JTA：Java Transaction API）和它的同胞Java事务服务（JTS：Java Transaction Service），为J2EE平台提供了分布式事务服务（distributed transaction）的能力。 某种程度上，可以认为JTA规范是XA规范的Java版，其把XA规范中规定的DTP模型交互接口抽象成Java接口中的方法，并规定每个方法要实现什么样的功能。 下面的例子基于 Atomikos TransactionEssentials 实现 JTA： 所需要依赖，这里用的 Gradle，用的 Maven 家人们可以自行找下依赖 123456789101112implementation platform('org.springframework.boot:spring-boot-dependencies:2.7.5')implementation 'com.atomikos:transactions-jdbc:6.0.0'implementation 'com.atomikos:transactions-jta:6.0.0'implementation 'javax.transaction:jta:1.1'implementation 'com.alibaba:druid:1.2.18'implementation 'org.springframework.boot:spring-boot-starter-web'implementation 'org.springframework.boot:spring-boot-test'implementation 'org.springframework.boot:spring-boot-starter-test'implementation 'org.mybatis.spring.boot:mybatis-spring-boot-starter:2.2.1'implementation 'org.springframework.boot:spring-boot'implementation 'org.springframework:spring-tx'implementation 'mysql:mysql-connector-java:8.0.33' 数据库配置，这里我在本地中启动了两个 Mysql 实例，数据库只要能支持 XA 协议即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * 这里用的Druid，其他连接池只要实现了javax.sql.XADataSource即可 */public abstract class AbstractDataSourceConfig &#123; protected DruidXADataSource createDruidXADataSource(String url, String username, String password) &#123; DruidXADataSource druidXADataSource = new DruidXADataSource(); //&lt;a href=\"https://github.com/alibaba/druid/wiki/DruidDataSource%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7%E5%88%97%E8%A1%A8\"/&gt; druidXADataSource.setUrl(url); druidXADataSource.setUsername(username); druidXADataSource.setPassword(password); druidXADataSource.setMaxActive(5); druidXADataSource.setInitialSize(1); druidXADataSource.setMaxWait(15000); druidXADataSource.setMinIdle(1); druidXADataSource.setTestOnBorrow(false); druidXADataSource.setTestWhileIdle(true); druidXADataSource.setValidationQuery(\"select 1\"); druidXADataSource.setTimeBetweenEvictionRunsMillis(60000); druidXADataSource.setMinEvictableIdleTimeMillis(300000); druidXADataSource.setAsyncInit(true); return druidXADataSource; &#125;&#125;/** * 这里是3306端口的实例配置 */ @Configuration@MapperScan(value = \"com.jl.demo.spring.repository.mapper.db3306\", sqlSessionFactoryRef = \"sqlSessionFactory3306\")public class DataSourceConfig3306 extends AbstractDataSourceConfig &#123; @Bean public DataSource dataSource3306() &#123; AtomikosDataSourceBean datasource = new AtomikosDataSourceBean(); DruidXADataSource druidXADataSource = createDruidXADataSource(\"jdbc:mysql://localhost:3306/xa_test\", \"root\", \"123456\"); datasource.setXaDataSource(druidXADataSource); //atomikos要求为每个AtomikosDataSourceBean名称 datasource.setUniqueResourceName(\"local_3306\"); return datasource; &#125; @Bean public SqlSessionFactory sqlSessionFactory3306() throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource3306()); PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); sqlSessionFactoryBean.setMapperLocations(resolver.getResources(\"classpath:/mybatis/db3306/*.xml\")); return sqlSessionFactoryBean.getObject(); &#125;&#125;/** * 这里是3308端口的实例配置 */ @Configuration@MapperScan(value = \"com.jl.demo.spring.repository.mapper.db3308\", sqlSessionFactoryRef = \"sqlSessionFactory3308\")public class DataSourceConfig3308 extends AbstractDataSourceConfig &#123; @Bean public DataSource dataSource3308() &#123; AtomikosDataSourceBean datasource = new AtomikosDataSourceBean(); DruidXADataSource druidXADataSource = createDruidXADataSource(\"jdbc:mysql://localhost:3308/xa_test\", \"root\", \"123456\"); datasource.setXaDataSource(druidXADataSource); datasource.setUniqueResourceName(\"local_3308\"); return datasource; &#125; @Bean public SqlSessionFactory sqlSessionFactory3308() throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource3308()); PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); sqlSessionFactoryBean.setMapperLocations(resolver.getResources(\"classpath:/mybatis/db3308/*.xml\")); return sqlSessionFactoryBean.getObject(); &#125;&#125; 事务相关配置，spring 已经帮我们把 JtaTransactionManager 封装好了，底层使用的是我们传入的事务管理器 1234567891011121314151617181920212223@Configurationpublic class TransactionConfig &#123; /** * 配置atomikos事务管理器 */ @Bean public UserTransactionManager userTransactionManager() &#123; UserTransactionManager userTransactionManager = new UserTransactionManager(); userTransactionManager.setForceShutdown(false); return userTransactionManager; &#125; /** * 配置jta事务管理器，底层使用atomikos事务管理器 */ @Bean public TransactionManager jtaTransactionManager() &#123; JtaTransactionManager jtaTransactionManager = new JtaTransactionManager(); jtaTransactionManager.setTransactionManager(userTransactionManager()); return jtaTransactionManager; &#125;&#125; 下面简单看一下 service 层，由于本例子主要是为了测试 Jta 是否能正确回滚两个数据库实例，所以用了两个相同的表，不含业务属性 1234567891011121314151617181920212223@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserMapper3306 userMapper3306; @Autowired private UserMapper3308 userMapper3308; @Override @Transactional(transactionManager = \"jtaTransactionManager\", rollbackFor = Exception.class) public void insert() &#123; User user3306 = new User(); user3306.setName(\"111\"); userMapper3306.insert(user3306); com.jl.demo.spring.repository.model.db3308.User user3308 = new com.jl.demo.spring.repository.model.db3308.User(); user3308.setName(\"222\"); userMapper3308.insert(user3308); throw new RuntimeException(\"test\"); &#125;&#125; 测试代码 123456789101112@ExtendWith(SpringExtension.class)@SpringBootTest(classes = AtomikosTestApplication.class)class UserServiceImplTest &#123; @Autowired private UserService userService; @Test void insert() &#123; userService.insert(); &#125;&#125; 仓库地址：https://github.com/JerryQTQcjl/distributed-transaction-demo/tree/master/atomikos-xa-demo 参考http://www.tianshouzhi.com/api/tutorials/distributed_transaction/386 https://github.com/atomikos/transactions-essentials","categories":[],"tags":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"聊聊双亲委派机制","slug":"聊聊双亲委派机制","date":"2023-04-29T17:17:16.000Z","updated":"2023-04-29T17:29:38.439Z","comments":true,"path":"2023/04/30/聊聊双亲委派机制/","link":"","permalink":"http://example.com/2023/04/30/%E8%81%8A%E8%81%8A%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%9C%BA%E5%88%B6/","excerpt":"","text":"最近重新看了双亲委派相关的知识，特此记录一下，方便以后重新回顾 Java 类是怎么加载Java 通过 ClassLoader 实例的 loadClass 方法将字节码（.class）文件加载到 JVM 的方法区中。啥也不说，先上代码 (～￣▽￣)～： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // 从已加载的类中寻找 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; //父委派机制 if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; //没有父类加载器，则说明父加载器是启动类加载器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; &#125; if (c == null) &#123; //根据类名从数据源查找对应的字节码，读取二进制流，生成 Class 对象 c = findClass(name); &#125; &#125; if (resolve) &#123; //官方注释说是链接一个类，这里暂时没用过，下次看看 resolveClass(c); &#125; return c; &#125; &#125;//此处贴的是 ClassLoader 子类 URLClassLoader 的 findClass 方法，也是默认的实现protected Class&lt;?&gt; findClass(final String name) throws ClassNotFoundException &#123; final Class&lt;?&gt; result; try &#123; result = AccessController.doPrivileged( new PrivilegedExceptionAction&lt;Class&lt;?&gt;&gt;() &#123; public Class&lt;?&gt; run() throws ClassNotFoundException &#123; //根据类名从资源路径查找对应的资源 String path = name.replace('.', '/').concat(\".class\"); Resource res = ucp.getResource(path, false); if (res != null) &#123; try &#123; //读取二进制流，生成 Class 对象 return defineClass(name, res); &#125; catch (IOException e) &#123; throw new ClassNotFoundException(name, e); &#125; &#125; else &#123; return null; &#125; &#125; &#125;, acc); &#125; catch (java.security.PrivilegedActionException pae) &#123; throw (ClassNotFoundException) pae.getException(); &#125; if (result == null) &#123; throw new ClassNotFoundException(name); &#125; return result; &#125; 从上面的代码可以看到 loadClass 主要做了两件事情： 根据类名从资源路径（文件、网络等）查找对应的字节码资源 从资源路径读取字节码二进制流，并生成对应的 Class 对象，其中包括权限校验，字节码校验等。 Java 默认的类加载器JDK9 之前默认的类加载器有：AppClassLoader、ExtClassLoader、BootstrapClassloader BootstrapClassLoader: JRE 的 lib 目录下 jar 包中的类（以及由虚拟机参数 -Xbootclasspath 指定的类），注意，他不是定义在 Java 代码中的。 ExtClassLoader：存放在 JRE 的 lib/ext 目录下 jar 包中的类（以及由系统变量 java.ext.dirs 指定的类）。 AppClassLoader：负责加载应用程序路径下的类（虚拟机参数 -cp/-classpath、系统变量 java.class.path 或环境变量 CLASSPATH 所指定的路径） JDK9 及以后默认的类加载器：AppClassLoader、PlatformClassLoader、BootstrapClassLoader PlatformClassLoader：其实就是之前的 ExtClassLoader，只是将大部分原本由 BootstrapClassLoader 加载的类交由 PlatformClassLoader 加载 Java 类的加载时机有哪些类加载的时机有很多，这里主要列举一些易错场景 (～￣▽￣)～： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ClassLoadTimingTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; //new一个对象实例 new A(); //加载A //通过反射实例化一个对象 Class.forName(\"com.classloader.demo.char01.A\"); //加载A //访问类的静态变量 System.out.println(A.word); //加载A //访问类的静态方法 A.println(); //加载A //子类初始化会触发父类初始化 System.out.println(B.word1); //加载A、B //以下情况不加载 System.out.println(B.word); //加载A，不加载B //创建数组不加载类 System.out.println(new A[]&#123;&#125;); //不加载A //访问常量不加载类 System.out.println(A.DEFAULT); //不加载A //易混淆情况，编译器无法确定最终变量的值，所以运行期要去加载 System.out.println(A.DEFAULT1); //加载A &#125;&#125;class A&#123; protected final static String DEFAULT = \"HELLO WORLD\"; protected final static String DEFAULT1 = new String(\"123\"); public static String word = \"HELLO WORLD\"; static &#123; System.out.println(\"A was loader\"); &#125; public static void println()&#123;&#125;&#125;class B extends A&#123; public static String word1 = \"HELLO WORLD\"; static &#123; System.out.println(\"B was loader\"); &#125;&#125; 什么是双亲委派机制仔细的同学应该会发现，上面加载类的方法中，优先会调用 parent.loadClass 去加载类，如果没加载到才会走继续往下走。 其实这就是类加载的父委派机制，优先由父加载器去加载类，父类加载器没加载到才有自身尝试去加载。 那么什么是双亲委派机制呢，还记得上面提到 JDK8 的默认类加载器有 AppClassLoader、ExtClassLoader、BootstrapClassloader，BootstrapClassloader 是 ExtClassLoader 的父加载器，ExtClassLoader 是 AppClassLoader 的父加载器。嗯，就是因为 AppClassLoader 有一个爸爸和一个爷爷，所以称为双亲委派机制。就是这么简单 ︿(￣︶￣)︿。 那么双亲委派机制的作用是什么呢，我认为有两点： 防止同一个类被加载到 JVM 多次 避免 Java 内部的一些基础类没有被正确加载，导致出现难以意料的异常 JDBC 真的打破双亲委派了吗在说 JDBC 是否打破双亲委派之前我们先来聊聊什么是打破双亲委派机制，嗯，就是让类的加载顺序不在是 BootstrapClassloader -&gt; ExtClassLoader -&gt; AppClassLoader，就是字面上的意思不再让爷爷和爸爸先加载，而是儿子自己想先加载就先加载 ︿(￣︶￣)︿。那么我们该怎么做呢，是不是自定义一个类加载器，重写下 loadClass 方法不在调用 parent.loadClass 就可以了？嗯，是的 o(￣▽￣)ｄ。 那么我们再来看看 JDBC 真的打破双亲委派了吗。老规矩，线上代码 (～￣▽￣)～： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071AppClassLoaderpublic class DriverManager &#123; static &#123; loadInitialDrivers(); println(\"JDBC DriverManager initialized\"); &#125; private static void loadInitialDrivers() &#123; String drivers; try &#123; //获取环境变量中配置的驱动 drivers = AccessController.doPrivileged(new PrivilegedAction&lt;String&gt;() &#123; public String run() &#123; return System.getProperty(\"jdbc.drivers\"); &#125; &#125;); &#125; catch (Exception ex) &#123; drivers = null; &#125; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; //通过SPI机制加载驱动类 ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); */ try&#123; while(driversIterator.hasNext()) &#123; //这里面会加载驱动类（不初始化），但是会通过反射实例化驱动类（会初始化） driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println(\"DriverManager.initialize: jdbc.drivers = \" + drivers); if (drivers == null || drivers.equals(\"\")) &#123; return; &#125; String[] driversList = drivers.split(\":\"); println(\"number of Drivers:\" + driversList.length); for (String aDriver : driversList) &#123; try &#123; println(\"DriverManager.Initialize: loading \" + aDriver); //加载驱动类并初始化 Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); &#125; catch (Exception ex) &#123; println(\"DriverManager.Initialize: load failed: \" + ex); &#125; &#125; &#125;&#125;public final class ServiceLoader&lt;S&gt; &#123; private final ClassLoader loader; private LazyIterator lookupIterator; public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; //拿到线程上下文中的类加载器，这里如果未设置过获取到的是 AppClassLoader ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); &#125;&#125; 从上面的代码可以看到，DriverManager 通过 SPI 机制，加载 Driver 驱动类，而 ServiceLoader 中其实只是拿到当前上下文中的类加载器，那么能说他打破了双亲委派机制吗？我个人觉得是不能的，先不说其他，在未手动设置线程上下文中的类加载器的情况下，线程上下文类加载器是继承至父线程的，其实也就是 AppClassLoader，那正常情况下他走的就是双亲委派机制 （￣︶￣）↗。就算是将自定义的类加载器放入线程上下文中，那也是由自定义的类加载器通过重写 loadClass 打破双亲委派机制呀，所以我认为 JDBC 自身并未打破了双亲委派机制 &lt;(￣︶￣)&gt;。 Tomcat 如何打破双亲委派那么 Tomcat 打破了吗？嗯，它打破了 ｂ（￣▽￣）ｄ，Tomcat7 及以上自定义了类加载器 ParallelWebappClassLoader 和 WebappClassLoader。嗯，这里注意一下，Tomcat6 以下的类加载器有所不同，不过原理都是一样的（其实是我偷懒，不想把 Tomcat6 源码也看了 (╯▽╰)）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (JreCompat.isGraalAvailable() ? this : getClassLoadingLock(name)) &#123; String resourceName = binaryNameToPath(name, false); //这里获取到 ExtClassLoader ClassLoader javaseLoader = getJavaseClassLoader(); boolean tryLoadingFromJavaseLoader; try &#123; URL url; if (securityManager != null) &#123; PrivilegedAction&lt;URL&gt; dp = new PrivilegedJavaseGetResource(resourceName); url = AccessController.doPrivileged(dp); &#125; else &#123; //先ExtClassLoader和BoostrapClassLoader的资源路径中查找字节码资源 url = javaseLoader.getResource(resourceName); &#125; //如果资源存在则走ExtClassLoader和BoostrapClassLoader加载 tryLoadingFromJavaseLoader = (url != null); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); tryLoadingFromJavaseLoader = true; &#125; if (tryLoadingFromJavaseLoader) &#123; try &#123; //如果资源存在则走ExtClassLoader和BoostrapClassLoader加载 clazz = javaseLoader.loadClass(name); //return &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; //判断是否需要委派给父类加载器，默认是AppClassLoader，或者是Tomcat内部的一些类 boolean delegateLoad = delegate || filter(name, true); // (1) Delegate to our parent if requested if (delegateLoad) &#123; try &#123; clazz = Class.forName(name, false, parent); //return &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; try &#123; //从本地目录中查找类并加载 clazz = findClass(name); //return &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; // (3) Delegate to parent unconditionally if (!delegateLoad) &#123; try &#123; //还是没有找到就委派给父加载器，默认是AppClassLoader clazz = Class.forName(name, false, parent); //return &#125; catch (ClassNotFoundException e) &#123; // Ignore &#125; &#125; &#125; throw new ClassNotFoundException(name);&#125; 我们看看 Tomcat 都改了啥： 首先尝试通过 ExtClassLoader 加载类，防止 java 相关的一些类被先加载了 判断是否需要委派给父类加载器，或者是是否是Tomcat内部的一些类，是的话则委派给父类加载器 从本地目录和扩展路径中查找类并加载，还记的早起我们使用 Tomcat 时，会将多个 war 包放在 webapps 吗，那么如果多个 war 中完全的全路径类名，走默认的双亲委派机制就只有先加载的能加载成功。此处便是 Tomcat 打破双亲委派意义了。 如果还是没有找到，就委派给父类加载器，走双亲委派机制。 可以看到，Tomcat 的类加载顺序不再是 BootstrapClassloader -&gt; ExtClassLoader -&gt; AppClassLoader，而是在 AppClassLoader 前先尝试通过 WebappClassLoader 加载本地目录 ｂ（￣▽￣）ｄ。 自定义类加载器篇幅有点长了，这个下次一定 ╰(￣▽￣)╭ 总结 Java 通过 ClassLoader 实例的 loadClass 方法将字节码（.class）文件加载到 JVM 的方法区中。 JDK9 之前默认的类加载器有：AppClassLoader、ExtClassLoader、BootstrapClassloader ,JDK9 及以后默认的类加载器：**AppClassLoader、PlatformClassLoader、BootstrapClassLoader **。 类加载时机包括：new一个对象实例；反射实例化一个对象；访问类的静态变量、方法；子类初始化会触发父类初始化；方法句柄调用加载方法所在类。 不加载时机：访问父类静态变量，不加载子类；创建数组不加载类；访问常量不加载类，但是如果编译是无法确定，运行期还是会加载类。 JVM 为防止同一个类被加载多次，引入双亲委派机制，加载类过程中，先由其父类加载器加载，未加载到在自己加载 JDBC 并未打破双亲委派，而 Tomcat 通过自定义 ClassLoader 打破双亲委派。","categories":[],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"}]},{"title":"Spring @Validated 失效分析","slug":"Spring-Validated-失效分析","date":"2023-04-21T17:41:36.000Z","updated":"2023-04-22T06:07:27.069Z","comments":true,"path":"2023/04/22/Spring-Validated-失效分析/","link":"","permalink":"http://example.com/2023/04/22/Spring-Validated-%E5%A4%B1%E6%95%88%E5%88%86%E6%9E%90/","excerpt":"","text":"最近在落地 DDD，希望对 command 进行参数校验，由于部分流量入口是 MQ，所以希望在应用层是用 @Validated 进行参数校验，结果。。。 Controller 中使用 @Validated@Validated 注解的作用这里就不多做介绍了，具体用法在网上应该有不少。 在之前使用 MVC 架构编码时，通常是将 @Validated 注解或者 @Valid 配置在 Controller 的方法中，如下代码所示： 12345@PostMapping(\"common/set\")public Response&lt;?&gt; setCommonSetting(@RequestBody @Validated SetCommonSettingReqVO reqVO) &#123; //doSomeThings return Response.success();&#125; 所以在配置应用层校验时，就想当然的按照类似的写法： 123public void addClueTrack(@Validated AddClueTrackCommand command) &#123; //doSomeThings&#125; 结果可想而知，@Validated 注解并不生效。 @Validated 是怎么生效的？竟然不生效，那么就开始分析原因。 首先可以很容易想到，竟然能在方法执行前就拦截进行校验，那么大概率是使用动态代理。就和 @Transactional 事务注解一样，底层都是基于 AOP 实现动态代理。 接下来为了印证这个想法，就是需要深入看看 Spring 实现的。通过 IDE 可以很方便看到有哪些地方引用了 @Validated 注解： 其中一个类名一下就引起了我的注意 MethodValidationPostProcessor，熟悉 Spring 的小伙伴应该知道，Spring 中有很多 BeanPostProcessor 用于扩展 Bean，Aop 便是基于此实现动态代理的。点进去一看，果不其然： 123456789101112131415161718192021222324252627282930313233343536public class MethodValidationPostProcessor extends AbstractBeanFactoryAwareAdvisingPostProcessor implements InitializingBean &#123; private Class&lt;? extends Annotation&gt; validatedAnnotationType = Validated.class; @Nullable private Validator validator; //... @Override public void afterPropertiesSet() &#123; //创建切点 Pointcut pointcut = new AnnotationMatchingPointcut(this.validatedAnnotationType, true); this.advisor = new DefaultPointcutAdvisor(pointcut, createMethodValidationAdvice(this.validator)); &#125; protected Advice createMethodValidationAdvice(@Nullable Validator validator) &#123; //创建拦截器 return (validator != null ? new MethodValidationInterceptor(validator) : new MethodValidationInterceptor()); &#125;&#125;public class AnnotationMatchingPointcut implements Pointcut &#123; private final ClassFilter classFilter; private final MethodMatcher methodMatcher; public AnnotationMatchingPointcut(Class&lt;? extends Annotation&gt; classAnnotationType, boolean checkInherited) &#123; //切点只针对类级别 this.classFilter = new AnnotationClassFilter(classAnnotationType, checkInherited); this.methodMatcher = MethodMatcher.TRUE; &#125; //...&#125; MethodValidationPostProcessor 中创建了一个切点，过滤类上添加了 @Validated 的 Bean，只要满足此条件，就会根据 MethodValidationInterceptor 生成对应的代理类。嗯，和 @Transactional 的实现原理差不多。 ok，看到这里我就在应用服务实现上添加了 @Validated 注解，那么此时注解生效了吗？哈哈，进度条还没过半呢😂 理论上类上加上 @Validated 注解，应该会生成动态代理类的，竟然没成功进行参数校验，我能想到的原因有二： 1. MethodValidationPostProcessor 没注入到 BeanFactory 中，所以没生成对应的代理类2. MethodValidationInterceptor 对还有其他需要满足的条件，而目前还未满足 这里先剧透一下，答案是 2 🌝 MethodValidationInterceptor 需要满足什么条件竟然答案是2，那这里就先讲一下 MethodValidationInterceptor，MethodValidationPostProcessor 是怎么注册到容器的咱们后面再来讲。 123456789101112131415161718192021222324252627282930313233343536373839ExecutableValidatorpublic class MethodValidationInterceptor implements MethodInterceptor &#123; private final Validator validator; @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable &#123; // Standard Bean Validation 1.1 API ExecutableValidator execVal = this.validator.forExecutables(); Method methodToValidate = invocation.getMethod(); Set&lt;ConstraintViolation&lt;Object&gt;&gt; result; //获取类本身的实例（非代理类），请记住这里，这里就是和 Controller 最大的区别 Object target = invocation.getThis(); Assert.state(target != null, \"Target must not be null\"); try &#123; //执行参数校验，校验的是当前类，也就是说校验的是 Bean 对应的类 result = execVal.validateParameters(target, methodToValidate, invocation.getArguments(), groups); &#125; catch (IllegalArgumentException ex) &#123; //doSomeThings &#125; if (!result.isEmpty()) &#123; throw new ConstraintViolationException(result); &#125; //执行方法 Object returnValue = invocation.proceed(); //校验返回值 result = execVal.validateReturnValue(target, methodToValidate, returnValue, groups); if (!result.isEmpty()) &#123; throw new ConstraintViolationException(result); &#125; return returnValue; &#125;&#125; 接下来就要看看 ExecutableValidator.validateParameters 这个方法是如何实现的，为了方便阅读，这里我只保留了部分核心代码。根据包名我们大概也能猜到 ExecutableValidator.validateParameters 是 hibernate-validator 包提供的方法，而 @Validated 注解是由 Spring 提供的，所以不生效也就正常了。接下来我们继续往下走，我这里只贴部分核心的代码，中间的栈路径可以根据以下这个路径往下走： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * --&gt; org.hibernate.validator.internal.engine.ValidatorImpl#validateParameters * --&gt; org.hibernate.validator.internal.metadata.BeanMetaDataManager#getBeanMetaData * --&gt; org.hibernate.validator.internal.metadata.BeanMetaDataManagerImpl#createBeanMetaData * --&gt; org.hibernate.validator.internal.metadata.BeanMetaDataManagerImpl#getBeanConfigurationForHierarchy * --&gt; org.hibernate.validator.internal.metadata.provider.MetaDataProvider#getBeanConfiguration * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#retrieveBeanConfiguration * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#getFieldMetaData * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#findPropertyMetaData * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#findConstraints * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#findCascadingMetaData * &lt;-- ... * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#getMethodMetaData * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#getConstructorMetaData * --&gt; org.hibernate.validator.internal.metadata.provider.AnnotationMetaDataProvider#getClassLevelConstraints * &lt;-- ... * --&gt; org.hibernate.validator.internal.metadata.aggregated.BeanMetaData#hasConstraints * --&gt; org.hibernate.validator.internal.engine.ValidatorImpl#validateParametersInContext * */public class ValidatorImpl implements Validator, ExecutableValidator &#123; @Override public final &lt;T&gt; Set&lt;ConstraintViolation&lt;T&gt;&gt; validateValue(Class&lt;T&gt; beanType, String propertyName, Object value, Class&lt;?&gt;... groups) &#123; Contracts.assertNotNull( beanType, MESSAGES.beanTypeCannotBeNull() ); sanityCheckPropertyPath( propertyName ); sanityCheckGroups( groups ); //获取 bean 及其父类、超类的 BeanMetaData&lt;T&gt; rootBeanMetaData = beanMetaDataManager.getBeanMetaData( beanType ); //判断该 bean 是否有约束 if ( !rootBeanMetaData.hasConstraints() ) &#123; return Collections.emptySet(); &#125; PathImpl propertyPath = PathImpl.createPathFromString( propertyName ); BaseBeanValidationContext&lt;T&gt; validationContext = getValidationContextBuilder().forValidateValue( beanType, rootBeanMetaData, propertyPath ); ValidationOrder validationOrder = determineGroupValidationOrder( groups ); //校验参数 return validateValueInContext(validationContext, value, propertyPath, validationOrder); &#125; //...&#125; 当我调试到 rootBeanMetaData.hasConstraints() 时，判断没有约束，然后就直接返回了没有进行参数校验。我就想说看看是如何判断 Bean 是否有约束的，于是就返回上层进入 beanMetaDataManager.getBeanMetaData 中看，结果发现里面的代码有够复杂的🌚 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class AnnotationMetaDataProvider implements MetaDataProvider &#123; //获取类上所有的约束条件 private &lt;T&gt; BeanConfiguration&lt;T&gt; retrieveBeanConfiguration(Class&lt;T&gt; beanClass) &#123; //获取字段上的约束条件 Set&lt;ConstrainedElement&gt; constrainedElements = getFieldMetaData( beanClass ); //获取方法上的约束条件（包括参数、返回值） constrainedElements.addAll( getMethodMetaData( beanClass ) ); //获取构造函数 constrainedElements.addAll( getConstructorMetaData( beanClass ) ); //获取类上的约束条件 Set&lt;MetaConstraint&lt;?&gt;&gt; classLevelConstraints = getClassLevelConstraints( beanClass ); if ( !classLevelConstraints.isEmpty() ) &#123; ConstrainedType classLevelMetaData = new ConstrainedType(ConfigurationSource.ANNOTATION, beanClass, classLevelConstraints); constrainedElements.add( classLevelMetaData ); &#125; return new BeanConfiguration&lt;&gt;(ConfigurationSource.ANNOTATION, beanClass, constrainedElements, getDefaultGroupSequence( beanClass ), getDefaultGroupSequenceProvider( beanClass )); &#125; //查找约束注解 protected &lt;A extends Annotation&gt; List&lt;ConstraintDescriptorImpl&lt;?&gt;&gt; findConstraintAnnotations(Constrainable constrainable, A annotation, ConstraintLocationKind type) &#123; //如果包含 \"jdk.internal\" and \"java\" 下的注解，则直接不进行校验 if ( constraintCreationContext.getConstraintHelper().isJdkAnnotation( annotation.annotationType() ) ) &#123; return Collections.emptyList(); &#125; List&lt;Annotation&gt; constraints = newArrayList(); Class&lt;? extends Annotation&gt; annotationType = annotation.annotationType(); //判断是否有约束条件，也就我们经常配置的 @NotNull，@Min 这类注解 if ( constraintCreationContext.getConstraintHelper().isConstraintAnnotation( annotationType ) ) &#123; constraints.add( annotation ); &#125; //这个没用过，暂时跳过 else if ( constraintCreationContext.getConstraintHelper().isMultiValueConstraint( annotationType ) ) &#123; constraints.addAll( constraintCreationContext.getConstraintHelper().getConstraintsFromMultiValueConstraint( annotation ) ); &#125; return constraints.stream() .map( c -&gt; buildConstraintDescriptor( constrainable, c, type ) ) .collect( Collectors.toList() ); &#125; //构建级联元数据构造器，也就是我们常用的 @Valid，在 Bean 中如果我们要对对象属性进行校验， //需要在该属性上添加 @Valid，此处便是如此 private CascadingMetaDataBuilder getCascadingMetaData(JavaBeanAnnotatedElement annotatedElement, Map&lt;TypeVariable&lt;?&gt;, CascadingMetaDataBuilder&gt; containerElementTypesCascadingMetaData) &#123; return CascadingMetaDataBuilder.annotatedObject( annotatedElement.getType(), annotatedElement.isAnnotationPresent( Valid.class ), containerElementTypesCascadingMetaData, getGroupConversions( annotatedElement.getAnnotatedType() ) ); &#125;&#125; 顺着上面的栈路径一直往下走，最终发现最核心的几个方法是 getFieldMetaData、getMethodMetaData、getConstructorMetaData、getClassLevelConstraints，这个几方法都是用于获取约束和级联元数据。那么里面到底是怎么获取约束元数据的呢，咱继续往里钻，可以看到最终调用了 findConstraintAnnotations 获取约束元数据，也就是我们平时用到的 @NotNull，@Min 等注解，通过 getCascadingMetaData 获取级联元数据，也就是 @Valid 注解。看到这，是不是很容易就能想到，知道我加上 @Valid 就能成功校验了呢？ 于是我尝试了一波，果然没问题。嗯~ 长见识了😂。由于时间有限，ValidatorImpl.validateParametersInContext() 方法我就没有深入进去看了。感兴趣的小伙伴可以自行去看看！！🌝 那么 Controller 为啥直接添加 @Validated 或者 @Valid 就可以呢？明白了在应用服务实现，准确的说应该是普通 Bean 中应该怎么配置之 @Validated 和 @Valid 使其生效之后，我就很好奇为啥 Controller 只需要单独在方法上配置 @Validated 或者 @Valid 就能成功校验呢？ 还记得上面通过 IDE 查看应用 @Validated 注解的类时，我们发现了 MethodValidationPostProcessor，还有另外几个类一看就很像 Controller 参数解析相关的类： 我在这几个类上各打了一个断点，最终进入的是 AbstractMessageConverterMethodArgumentResolver。 ok，那就看看他是怎么实现的，这里只贴了很参数校验相关的方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public abstract class AbstractMessageConverterMethodArgumentResolver implements HandlerMethodArgumentResolver &#123; protected void validateIfApplicable(WebDataBinder binder, MethodParameter parameter) &#123; Annotation[] annotations = parameter.getParameterAnnotations(); for (Annotation ann : annotations) &#123; //获取分组信息 Object[] validationHints = ValidationAnnotationUtils.determineValidationHints(ann); if (validationHints != null) &#123; //进行校验 binder.validate(validationHints); break; &#125; &#125; &#125;&#125;public abstract class ValidationAnnotationUtils &#123; @Nullable public static Object[] determineValidationHints(Annotation ann) &#123; Class&lt;? extends Annotation&gt; annotationType = ann.annotationType(); String annotationName = annotationType.getName(); //如果是 @valid 注解直接返回一个空数组 if (\"javax.validation.Valid\".equals(annotationName)) &#123; return EMPTY_OBJECT_ARRAY; &#125; //如果是 @validated 则返回其分组信息 Validated validatedAnn = AnnotationUtils.getAnnotation(ann, Validated.class); if (validatedAnn != null) &#123; Object hints = validatedAnn.value(); return convertValidationHints(hints); &#125; if (annotationType.getSimpleName().startsWith(\"Valid\")) &#123; Object hints = AnnotationUtils.getValue(ann); return convertValidationHints(hints); &#125; return null; &#125;&#125;public class DataBinder implements PropertyEditorRegistry, TypeConverter &#123; public void validate(Object... validationHints) &#123; //此处是关键所在，这里获取的是参数！！！和普通的 Bean 获取到的却是 Bean 本身 Object target = getTarget(); Assert.state(target != null, \"No target to validate\"); BindingResult bindingResult = getBindingResult(); // Call each validator with the same binding result for (Validator validator : getValidators()) &#123; if (!ObjectUtils.isEmpty(validationHints) &amp;&amp; validator instanceof SmartValidator) &#123; ((SmartValidator) validator).validate(target, bindingResult, validationHints); &#125; else if (validator != null) &#123; validator.validate(target, bindingResult); &#125; &#125; &#125;&#125; 可以看到，对于 Controller 不论是直接在参数上加上 @Validated 或者 @Valid 注解，都会进入到校验方法，而且校验的就是参数！！！而 Bean 校验的却是 Bean 本身！！！ MethodValidationPostProcessor 和 AbstractMessageConverterMethodArgumentResolver 是怎么被注册到 BeanFactory 的？明白了 @Validated 的拦截实现的原理后，那么就只剩最后一个问题了，MethodValidationPostProcessor 和 AbstractMessageConverterMethodArgumentResolver 是怎么被注册到 BeanFactory 的。 其实不用看源码大概有也能猜到是 Spring Boot 自动装配的。为了印证一下，我还是贴一下源码： 123456789101112131415161718@Configuration(proxyBeanMethods = false)@ConditionalOnClass(ExecutableValidator.class)@ConditionalOnResource(resources = \"classpath:META-INF/services/javax.validation.spi.ValidationProvider\")@Import(PrimaryDefaultValidatorPostProcessor.class)public class ValidationAutoConfiguration &#123; //... @Bean @ConditionalOnMissingBean public static MethodValidationPostProcessor methodValidationPostProcessor(Environment environment, @Lazy Validator validator, ObjectProvider&lt;MethodValidationExcludeFilter&gt; excludeFilters) &#123; FilteredMethodValidationPostProcessor processor = new FilteredMethodValidationPostProcessor(excludeFilters.orderedStream()); boolean proxyTargetClass = environment.getProperty(\"spring.aop.proxy-target-class\", Boolean.class, true); processor.setProxyTargetClass(proxyTargetClass); processor.setValidator(validator); return processor; &#125;&#125; 另外就是 AbstractMessageConverterMethodArgumentResolver 的几个实现类，均由 RequestMappingHandlerAdapter 实例化，而 RequestMappingHandlerAdapter 大家知道有 WebMvcAutoConfiguration 自动装配，时间原因，这就不看了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class RequestMappingHandlerAdapter extends AbstractHandlerMethodAdapter implements BeanFactoryAware, InitializingBean &#123; private List&lt;HandlerMethodArgumentResolver&gt; getDefaultArgumentResolvers() &#123; List&lt;HandlerMethodArgumentResolver&gt; resolvers = new ArrayList&lt;&gt;(30); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ServletModelAttributeMethodProcessor(false)); resolvers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestPartMethodArgumentResolver(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestHeaderMethodArgumentResolver(getBeanFactory())); resolvers.add(new RequestHeaderMapMethodArgumentResolver()); resolvers.add(new ServletCookieValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new ExpressionValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new SessionAttributeMethodArgumentResolver()); resolvers.add(new RequestAttributeMethodArgumentResolver()); // Type-based argument resolution resolvers.add(new ServletRequestMethodArgumentResolver()); resolvers.add(new ServletResponseMethodArgumentResolver()); resolvers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RedirectAttributesMethodArgumentResolver()); resolvers.add(new ModelMethodProcessor()); resolvers.add(new MapMethodProcessor()); resolvers.add(new ErrorsMethodArgumentResolver()); resolvers.add(new SessionStatusMethodArgumentResolver()); resolvers.add(new UriComponentsBuilderMethodArgumentResolver()); if (KotlinDetector.isKotlinPresent()) &#123; resolvers.add(new ContinuationHandlerMethodArgumentResolver()); &#125; // Custom arguments if (getCustomArgumentResolvers() != null) &#123; resolvers.addAll(getCustomArgumentResolvers()); &#125; // Catch-all resolvers.add(new PrincipalMethodArgumentResolver()); resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), true)); resolvers.add(new ServletModelAttributeMethodProcessor(true)); return resolvers; &#125;&#125; 小结1、在普通 Bean 中如果要通过注解的方式使用 hibernate-validator 进行校验的话，需要在类上添加 @Validated 注解，同时在方法上添加 @Valid 注解。或者也可以直接使用 @NotNull 等注解。 2、普通 Bean 使用 @Validated 是通过动态代理完成的。具体的拦截器便是他 MethodValidationInterceptor。 3、Controller 层之所以能 @Validated 和 @Valid 二选一，是因为校验的是参数本身，而普通 Bean 校验的是 Bean 本身。 4、至此，相信大家就不会没配置好 @Validated 导致失效了。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"kafka 消费者高 cpu 问题排查","slug":"kafka-消费者高-cpu-问题排查","date":"2023-04-14T15:31:22.000Z","updated":"2023-04-14T15:48:38.289Z","comments":true,"path":"2023/04/14/kafka-消费者高-cpu-问题排查/","link":"","permalink":"http://example.com/2023/04/14/kafka-%E6%B6%88%E8%B4%B9%E8%80%85%E9%AB%98-cpu-%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","excerpt":"","text":"今天本来打算愉快的划水，运维小哥突然找我说测试环境应用 cpu 一直居高不下，我一看告警还真是… cpu 高问题排查思路首先还是老套路： 先查看 cpu 高的线程是哪些 1top -Hp &lt;pid&gt; 查看线程的堆栈信息 12345//将线程 id 转换为 16 进制printf '%x\\n' &lt;tid&gt;//获取线程号后 50 行堆栈信息jstack &lt;pid&gt; | grep &lt;tid&gt; -A 50 这里我就直接用线程名称去查了。 看堆栈信息定位到是 kafka 消费者消费消息，导致 cpu 居高不下，正常情况下 kafka 消费者 cpu 飚高都是有大量的消息，我第一个感觉就是测试在进行压测，结果一看，打脸了🤔。 可以看到，lag 是 0 或者负数，我又刷新了几次基本上没有多少消息，这里留个心眼，后面我们在好好唠唠。 那么问题就来了，没有消息为啥消费者 cpu 会飚高… 突然灵机一动，会不会是消费者数太多了，导致循环去调用 poll 方法，造成整个节点 cpu 飚高。然后就屁颠屁颠的把所有主题的消费者数调小了，然鹅想想很美好，现实很骨感… 重启后节点的 cpu 依然居高不下。 遇到不懂就问度娘在网上逛了一圈，看到许多相似的场景，各种操作都试了一遍，还是没什么用🙃，kafka 的 github 仓库我也去逛了一圈，发现也有很多 cpu 高得场景，大多数都是建议升级客户端版本，忘了说我司目前用的还是 0.11.2，总之逛了一圈没有起到太大的帮助。 接着就是一波虾皮操作，显示生成了火焰图，看看 consumer poll 到底为啥一直占用 cpu，下面是一张火焰图： 虽然 poll 方法占用 cpu 耗时很长，但是仔细看又觉得没啥问题，是正常的在处理网络请求，这个时候我甚至一度怀疑是 kafka 客户端的 bug 😂。 有耐心问题迟早能解决还记的我们之前提了一嘴那张截图吗，没错就是下面这张 图中 lag 出现负数，其实 lag 出现负数还是很常见的，但问题就出在我排查了这么久图中的 lag 好像没变化过，而且一直是负数，那就很值得注意了。 我们还是先说说 lag 是怎么计算的 1lag = HW - consumerOffset HW: 高水位，通常等于所有 ISR 中最小的 LEO，详细的可以看看大佬的博客 consumerOffset: 表示消费者提交的消费位移 那么 lag 为啥会出现负数呢，由于我本身并未看过源码，所以从网上找了一个我认为比较是能说的通的解释: Producer 的 offset 是通过 JMX 轮询获得的，Consumer 的 offset 是从 kafka 内的 __consumer_offsets 的 topic 中直接读取到的，很明显轮询获取 offset 比 直接从 topic 拿 offset 慢一点，也就可能会出现 Lag 计算后为负数的情况。 OK，回到正题，lag 长时间是负数说明 consumerOffset 一直大于 HW，那么出现这个问题的原因大概率是 HW 一直不更新，因为 HW 只要更新其实 lag 很快就能变回 0。那么 HW 是什么时候更新的呢，其实是 Follower 副本同步 Leader 副本数据时，Leader 副本会对比 Follower 拉取数据的 offset 和 Leader 自身的 LEO 去更新 HW，所以通常 HW 需要 Follower 多同步一轮才会更新。 那么 HW 不更新，只能说明 Follower 没有去同步数据，想到这，我立马去看了下消费组的副本状态，发现有一个 broker 所有的分区副本都不在 ISR 中。那么基本上确定这个 broker 是出现问题了，但是这和我消费者 cpu 高有什么关系呢？ 这时运维小哥告诉我，poll 平均每 3ms 就请求一次，导致 cpu 飚高。纳尼？？？我的 poll timeout 明明是 100ms，怎么 3ms 一次呢，这明显有问题呀，运维小哥发了一下抓包的截图给我： kafka broker response 中提示 Not Leader For Partition，这不就和上面的猜想对上了吗，看看 chatgpt 给出的解释: 至此问题就排查了差不多了，那么接下来就是解决。由于是在测试环境发现的，解决方式也很粗暴，就是直接把 topic 直接删除了，然后重新创建。 小结 遇到涉及网络相关的问题，可以抓个包瞧瞧，说不定思路一下就打开了。 如果实在生产环境遇到这类问题，那么该怎么处理 broker 呢，这个得好好琢磨琢磨，目前思路是从 controller 下手。","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"wireshark 抓包 java应用中的 https 请求","slug":"wireshark-抓包-java应用中的-https-请求","date":"2023-04-03T07:08:41.000Z","updated":"2023-04-03T07:12:39.504Z","comments":true,"path":"2023/04/03/wireshark-抓包-java应用中的-https-请求/","link":"","permalink":"http://example.com/2023/04/03/wireshark-%E6%8A%93%E5%8C%85-java%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84-https-%E8%AF%B7%E6%B1%82/","excerpt":"","text":"最近生产中请求第三方接口的服务频繁告警，接口响应过慢，我们这边使用的 httpclient 连接池，各项配置看起来都没太大的问题，于是就想着说抓包看看是否有网络层面的问题还是的确是第三方接口慢。 通过 jsslkeylog 获取 sslkeylogwireshark 中 https 显示密文的样子有用过 wireshark 抓包 https 的大佬应该都知道，https 是有加密的，直接用 wireshark 抓包展示的全都是密文，如下图：可以看到，具体的 https 请求数据都被加密了。 如何让 https 在 wireshark 显示明文wireshark 中解密 https 请求的方式有多种，这里使用的方式是获取 https 请求时的 sslkeylog，使用到了一个 javaagent 工具 jsslkeylog，通过修改字节码达到发送 https 请求时将使用到的 sslkeylog 写入到本地磁盘的效果。具体流程也很简单：1、下载 jsslkeylog jar 包2、启动命令中加入 -javaagent:/path_to_jar/jSSLKeyLog.jar=/path_to_log/sslkeylog.log3、启动应用发起 https 请求4、之后应该就会在配置的 /path_to_log 中看到对应的 sslkeylog5、之后将 log 配置到 wireshark 中，prfferences -&gt; protocols -&gt; tls配置完成之后，就能看到原本的密文已经变成明文了，之后就能愉快的分析了🌝","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"k8s 搭建","slug":"k8s-搭建","date":"2023-03-31T15:45:37.000Z","updated":"2023-03-31T15:46:40.276Z","comments":true,"path":"2023/03/31/k8s-搭建/","link":"","permalink":"http://example.com/2023/03/31/k8s-%E6%90%AD%E5%BB%BA/","excerpt":"","text":"k8s搭建说明本文系搭建 kubernetes v1.21.3 版本集群笔记，使用三台虚拟机作为 CentOS7.9 系统测试机，安装 kubeadm、kubelet、kubectl 均使用 yum 安装，网络组件选用的是 flannel。 环境准备部署集群没有特殊说明均使用 root 用户执行命令。 2.1 硬件信息 ip hostname mem disk explain 192.168.85.2 192.168.85.2 2G 40GB k8s 控制平面节点 192.168.85.3 192.168.85.3 2G 40GB k8s 执行节点1 192.168.85.4 192.168.85.4 2G 40GB k8s 执行节点2 2.2 软件信息 12345software versionCentOS CentOS Linux release 7.9.2009 (Core)Kubernetes 1.21.3Docker 20.10.8Kernel 5.4.138-1.el7.elrepo.x86_64 2.3 禁用 swapswap 仅当内存不够时会使用硬盘块充当额外内存，硬盘的 io 较内存差距极大，禁用 swap 以提高性能各节点均需执行： 123swapoff -a cp /etc/fstab /etc/fstab.bakcat /etc/fstab.bak | grep -v swap &gt; /etc/fstab 2.4 关闭 SELinux 关闭 SELinux，否则 kubelet 挂载目录时可能报错 Permission denied，可以设置为 permissive 或 disabled，permissive 会提示 warn 信息各节点均需执行： 12setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 2.5 设置时区、同步时间 12timedatectl set-timezone Asia/Shanghai systemctl enable --now chronyd 查看同步状态： 1timedatectl status 2.6 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld 安装必要依赖1yum install -y yum-utils device-mapper-persistent-data lvm2 3.1 添加 aliyun docker-ce yum 源 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 重建 yum 缓存 1yum makecache fast 3.2 安装 Docker 查看可用 docker 版本 1yum list docker-ce.x86_64 --showduplicates | sort -r ==安装指定版本 Docker== yum install -y docker-ce-20.10.14-3.el7 这里以安装 20.10.14 版本举例，注意版本号不包含 : 与之前的数字。 3.3 确保网络模块开机自动加载 12lsmod | grep overlay lsmod | grep br_netfilter 若上面命令无返回值输出或提示文件不存在，需执行以下命令： 123456cat &gt; /etc/modules-load.d/docker.conf &lt;&lt;EOF overlay br_netfilter EOFmodprobe overlay modprobe br_netfilter 3.4 使桥接流量对 iptables 可见各个节点均需执行： 12345cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 验证是否生效，均返回 1 即正确。 12sysctl -n net.bridge.bridge-nf-call-iptables sysctl -n net.bridge.bridge-nf-call-ip6tables 3.5 配置 Docker 1mkdir /etc/docker 修改 cgroup 驱动为 systemd [k8s官方推荐]、限制容器日志量、修改存储类型，最后的 docker 家目录可修改： 123456789101112131415cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"registry-mirrors\": [\"https://gp8745ui.mirror.aliyuncs.com\"], \"data-root\": \"/data/docker\"&#125;EOF 服务脚本第 13 行修改： 123vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --default-ulimit core=0:0systemctl daemon-reload 添加开机自启，立即启动： 1systemctl enable --now docker 3.6 验证 Docker 是否正常 查看docker信息，判断是否与配置一致 部署 Kubernetes 集群如未说明，各节点均需执行如下步骤： 4.1 添加 kubernetes 源 123456789cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 重建yum缓存，输入y添加证书认证 1yum makecache fast 4.2 安装 kubeadm、kubelet、kubectl各节点均需安装 kubeadm、kubelet 12345yum list docker-ce.x86_64 --showduplicates | sort -rversion=1.21.3-0yum install -y kubelet-v1.21.0 kubeadm-v1.21.0 kubectl-v1.21.0systemctl enable kubelet --now 4.3 配置自动补全命令 安装 bash 自动补全插件 1yum install bash-completion -y 设置 kubectl 与 kubeadm 命令补全，下次 login 生效 12kubectl completion bash &gt;/etc/bash_completion.d/kubectlkubeadm completion bash &gt; /etc/bash_completion.d/kubeadm 4.4 为 Docker 设定使用的代理服务(暂跳过该步骤，由阿里云镜像解决)Kubeadm 部署 Kubernetes 集群的过程中，默认使用 Google 的 Registry 服务 k8s.gcr.io 上的镜像，例如k8s.grc.io/kube-apiserver 等，但国内无法访问到该服务。必要时，可自行设置合适的代理来获取相关镜像，或者从 Dockerhub 上下载镜像至本地后自行对镜像打标签。 这里简单说明一下设置代理服务的方法。编辑 /lib/systemd/system/docker.service 文件，在 [Service] 配置段中添加类似如下内容，其中的 PROXY_SERVER_IP 和 PROXY_PORT 要按照实际情况修改。 123Environment=\"HTTP_PROXY=http://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"HTTPS_PROXY=https://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"NO_PROXY=192.168.4.0/24\" 配置完成后需要重载 systemd，并重新启动 docker 服务： 12systemctl daemon-reloadsystemctl restart docker.service 需要特别说明的是，由 kubeadm 部署的 Kubernetes 集群上，集群核心组件 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd 等均会以静态 Pod 的形式运行，它们所依赖的镜像文件默认来自于 k8s.gcr.io 这一 Registry 服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种，本示例将选择使用更易于使用的前一种方式： 使用能够到达该服务的代理服务使用国内的镜像服务器上的服务，例如 gcr.azk8s.cn/google_containers 和 registry.aliyuncs.com/google_containers 等（经测试，v1.22.0 版本已停用）4.5 查看指定 k8s 版本需要哪些镜像 123456789kubeadm config images list --kubernetes-version v1.21.0k8s.gcr.io/kube-apiserver:v1.21.0k8s.gcr.io/kube-controller-manager:v1.21.0k8s.gcr.io/kube-scheduler:v1.21.0k8s.gcr.io/kube-proxy:v1.21.0k8s.gcr.io/pause:3.4.1k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.0 4.6 拉取镜像 1vim pullimages.sh 1234567891011121314151617181920212223#!/bin/bash# pull images ver=v1.21.0registry=registry.cn-hangzhou.aliyuncs.com/google_containersimages=`kubeadm config images list --kubernetes-version=$ver |awk -F '/' '&#123;print $2&#125;'` for image in $imagesdoif [ $image != coredns ];then docker pull $&#123;registry&#125;/$image if [ $? -eq 0 ];then docker tag $&#123;registry&#125;/$image k8s.gcr.io/$image docker rmi $&#123;registry&#125;/$image else echo \"ERROR: 下载镜像报错，$image\" fielse docker pull coredns/coredns:1.8.0 docker tag coredns/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0 docker rmi coredns/coredns:1.8.0fidone 4.7 修改 kubelet 配置默认 cgroup driver 123456mkdir /var/lib/kubeletcat &gt; /var/lib/kubelet/config.yaml &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemdEOF 4.8 初始化 master 节点仅 192.168.85.2 节点需要执行此步骤。 4.8.1 生成 kubeadm 初始化配置文件[可选] 仅当需自定义初始化配置时用。 1kubeadm config print init-defaults &gt; kubeadm-config.yaml 修改配置文件： 12localAPIEndpoint: advertiseAddress: 1.2.3.4 替换为： 1234localAPIEndpoint: advertiseAddress: 192.168.85.2 name: 192.168.85.2kubernetesVersion: 1.21.0 123networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 替换为： 1234kubernetesVersion: 1.21.0networking: podSubnet: \"10.244.0.0/16\" serviceSubnet: 10.96.0.0/12 4.8.2 测试环境是否正常 1kubeadm init phase preflight 4.8.3 初始化 master10.244.0.0/16 是 flannel 固定使用的 IP 段，设置取决于网络组件要求。 1kubeadm init --config=kubeadm-config.yaml --ignore-preflight-errors=2 --upload-certs | tee kubeadm-init.log 4.8.4 为日常使用集群的用户添加 kubectl 使用权限 123456su - iuskyemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/admin.confsudo chown $(id -u):$(id -g) $HOME/.kube/admin.confecho \"export KUBECONFIG=$HOME/.kube/admin.conf\" &gt;&gt; ~/.bashrcexit 4.8.5 配置 master 认证 12echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' &gt;&gt; /etc/profile . /etc/profile 如果不配置这个，会提示如下输出：The connection to the server localhost:8080 was refused - did you specify the right host or port?此时 master 节点已经初始化成功，但是还未安装网络组件，还无法与其他节点通讯。 4.8.6 安装网络组件以 flannel 为例： 1234curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull quay.io/coreos/flannel:v0.14.0kubectl apply -f kube-flannel.yml 4.8.7 查看 192.168.85.2 节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 如果 STATUS 提示 NotReady，可以通过 kubectl describe node 192.168.85.2 查看具体的描述信息，性能差的服务器到达 Ready 状态时间会长些。 4.9 初始化 node 节点并加入集群4.9.1 获取加入 kubernetes 的命令访问 192.168.85.2 输入创建新 token 命令： 1kubeadm token create --print-join-command 同时输出加入集群的命令： 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 这个 token 也可以使用上述 master 上执行的初始化输出结果。 4.9.2 在 node 节点上执行加入集群的命令 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 4.10 查看集群节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 4.11 部署 Dashboard4.11.1 部署 1curl -o recommended.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 默认 Dashboard 只能集群内部访问，修改 Service 为 NodePort 类型，暴露到外部： 12345678910111213141516vi recommended.yamlkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: ports: - port: 443 targetPort: 8443 nodePort: 30001 type: NodePort selector: k8s-app: kubernetes-dashboard 12345678910111213kubectl apply -f recommended.yaml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull kubernetesui/dashboard:v2.3.1docker pull kubernetesui/metrics-scraper:v1.0.6kubectl apply -f recommended.yamlkubectl get pods,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 0/1 ContainerCreating 0 52spod/kubernetes-dashboard-67484c44f6-shtz7 0/1 ContainerCreating 0 52sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 52sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 53s 查看状态正在创建容器中，稍后再次查看： 123456NAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 1/1 Running 0 2m11spod/kubernetes-dashboard-67484c44f6-shtz7 1/1 Running 0 2m11sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 2m11sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 2m12s 访问地址：https://NodeIP:30001；使用 Firefox 浏览器，Chrome 浏览器打不开不信任 SSL 证书的网站。 创建 service account 并绑定默认 cluster-admin 管理员集群角色： 123kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/&#123;print $1&#125;') 这里需要注意粘贴的时候有可能被换行，如果被换行，可在记事本中设置为一行。 使用输出的 token 登录 Dashboard。 问题解决1、dial tcp 10.96.0.1:443: connect: no route to host 123456systemctl stop dockersystemctl stop kubeletiptables --flushiptables -tnat --flushsystemctl start kubeletsystemctl start docker 2、failed to delegate add: failed to set bridge addr: “cni0“ already has an IP address different from 1 1https://blog.csdn.net/weixin_42562106/article/details/123749291 3、failed to add vxlanRoute (10.244.0.0/24 -&gt; 10.244.0.0): network is down 1234ip link delete flannel.1systemctl restart networkkubectl delete -f kube-flannel.ymlkubectl apply -f kube-flannel.yml 4、Get http://10.244.0.3:8181/ready: dial tcp 10.244.0.3:8181: connect: connection refused 12#重新corednskubectl -n kube-system rollout restart deployment/coredns 以上几个问题遇到可以先尝试重启所有的机器，如果不行在通过上述方案解决 参考：https://www.iuskye.com/2021/08/10/k8s-kubeadm-1213.html","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"redisson3.15.2 公平锁任务丢失","slug":"redisson3.15.2 公平锁任务丢失","date":"2023-03-31T15:22:45.000Z","updated":"2023-03-31T15:37:33.829Z","comments":true,"path":"2023/03/31/redisson3.15.2 公平锁任务丢失/","link":"","permalink":"http://example.com/2023/03/31/redisson3.15.2%20%E5%85%AC%E5%B9%B3%E9%94%81%E4%BB%BB%E5%8A%A1%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"场景： 线索流转改成 redisson 公平锁，任务耗时长，导致部分任务丢失。 一、redisson 公平锁实现1、redis 中的 K/V12345678list: redisson_lock_queue:&#123;test&#125; elem: UUID:threadIdzset: redisson_lock_timeout:&#123;key&#125; elem: UUID:threadId score: timeout = ttl + 线程等待时间(5*60000ms) + 当前时间戳hset: key hashKey: UUID:threadId hashVal: 1 2、上锁流程（这里就不放源码了，感兴趣可以自己看看） org.redisson.RedissonFairLock#tryLockInnerAsync 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 如果当前没有线程占用锁，则上锁 (ttl = watchdogtimeout)，同时redisson_lock_timeout:{key} 中所有的 score - 线程等待时间(5*60000ms) 如果存在锁，且是被当前线程占用的，则 hashVal 加一 如果存在锁，且不是被当前线程占用的，同时已经加入过等待队列，则返回当前线程在队列中的 ttl 如果存在锁，且不是被当前线程占用的，并且未加入等待队列，则加入等待队列，timeout= ttl + 线程等待时间(5*60000ms) + 当前时间戳，ttl 为队列中最后一个元素的 timeout - current 或者 锁的超时时间 3、订阅 redisson_lock__channel:{fairLock}:UUID:threadId此处阻塞线程，直到消息队列中有消息发送为止 4、解锁流程 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 判断锁是否存在，如果不存在则说明锁已经被释放了，判断等待队列中是否有elem，有的话取出第一个 publish message 如果锁存在，且非本线程持有，则直接返回 null 如果所存在，且是当前线程持有，并且获取所次数大于 1，则 hashVal减一，更新锁超时时间 如果所存在，且是当前线程持有，释放锁，判断等待队列中是否有elem，有的话取出第一个 publish message 5、获取到锁之后，取消订阅 redisson_lock__channel:{fairLock}:UUID:threadId 二、问题分析1、由于测试环境第三方鉴权接口较慢，每分配一条线索需要3-4s 2、存量线索有 1500+ 条，每 200 条作为一个任务，总共拆分 8 个任务，每个任务需要执行 200*3 = 600+ s 3、redisson 公平锁线程等待时间 (5*60000ms)，也就是说单个任务执行完至少会有一个任务过期，在 unlock 或者 lock 操作是会先清除过期任务 4、由于获取不到锁，线程会订阅 redisson_lock__channel:{fairLock}:UUID:threadId，阻塞知道接收到消息，又由于等待队列里的线程被清了，这个消息队列永远不会收到消息，所以线程一直阻塞，且任务无法执行，资源被占用。 5、代码模拟： 1234567891011121314151617public static void main(String[] args) &#123; Config config = new Config(); config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\"); Redisson redissonClient = (Redisson) Redisson.create(config); IntStream.range(0, 10).forEach(i -&gt; &#123; new Thread(() -&gt; &#123; //RedissonFairLock lock = (RedissonFairLock) redissonClient.getFairLock(\"test\"); RedissonFairLock lock = new RedissonFairLock(redissonClient.getCommandExecutor(), \"test\", 2000); lock.lock(); //最终输出次数小于10 System.out.println(i); ThreadUtil.sleep(10000); lock.unlock(); &#125;).start(); &#125;); &#125; 参考: Redisson分布式锁之公平锁原理","categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"apollo 集群架构","slug":"apollo-集群架构","date":"2023-03-25T18:25:23.000Z","updated":"2023-03-31T15:44:48.277Z","comments":true,"path":"2023/03/26/apollo-集群架构/","link":"","permalink":"http://example.com/2023/03/26/apollo-%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/","excerpt":"","text":"apollo 集群架构apollo 组成结构apollo 集群主要有三个部分组成，config-service，admin-service，portal config-service: 主要为应用客户端提供服务，包括配置的读取、推送等功能，config-service 内置 eureka，已提供 admin-service, portal 的服务发现、服务注册 admin-service: 主要为 apollo-portal 提供服务，包括应用配置管理、发布等功能 portal: 是 apollo 提供的服务配置前端页面 apollo 部署方案这个官网提供的高可用双环境架构，更多的架构方案可参考 docker-compose 部署 apollo 集群 执行 sql 脚本，生成 apollo 需要的 db 表 编写 docker-compose 文件，本次只搭建开发环境模拟单机单环境，config-service，admin-service，portal 都各自部署一个容器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: '2'networks: net:services: #开发环境configService，eureka apollo-configservice-dev: image: apolloconfig/apollo-configservice container_name: configservice-dev ports: - 8080:8080 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - eureka.instance.ip-address=192.168.0.103 networks: - net apollo-adminservice-dev: image: apolloconfig/apollo-adminservice container_name: adminservice-dev ports: - 8090:8090 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 #这里需要配置开环环境的configService - eureka.service.url=http://configservice-dev:8080/eureka/ depends_on: - apollo-configservice-dev networks: - net apollo-portal: image: apolloconfig/apollo-portal container_name: apollo-portal ports: - 8070:8070 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloPortalDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - spring.profiles.active=auth - APOLLO_PORTAL_ENVS=dev,fat,pre,gray,pro - DEV_META=http://configservice-dev:8080 # 以下这些暂时使用开发环境的 - FAT_META=http://configservice-dev:8080 - PRE_META=http://configservice-dev:8080 - GRAY_META=http://configservice-dev:8080 - PRO_META=http://configservice-dev:8080 depends_on: - apollo-adminservice-dev networks: - net","categories":[],"tags":[{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"consul 集群搭建及注意事项","slug":"consul-集群搭建及注意事项","date":"2023-03-25T17:16:50.000Z","updated":"2023-03-31T15:45:02.909Z","comments":true,"path":"2023/03/26/consul-集群搭建及注意事项/","link":"","permalink":"http://example.com/2023/03/26/consul-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","excerpt":"","text":"集群搭建本次利用 docker 搭建 consul 集群，利用 docker-compose 统一管理 集群包含三个 server agent: node1、node2、node3 集群包含两个 client agent: node4、node5，client agent 提供 ui **1、下载 docker 镜像 ** 1docker pull docker 2、编辑 docker-compose.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253version: '2'networks: byfn:services: consul1: image: consul container_name: node1 command: agent -server -bootstrap-expect=3 -node=node1 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 networks: - byfn consul2: image: consul container_name: node2 command: agent -server -retry-join=node1 -node=node2 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul3: image: consul container_name: node3 command: agent -server -retry-join=node1 -node=node3 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul4: image: consul container_name: node4 command: agent -retry-join=node1 -node=ndoe4 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8500:8500 depends_on: - consul2 - consul3 networks: - byfn consul5: image: consul container_name: node5 command: agent -retry-join=node1 -node=ndoe5 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8501:8500 depends_on: - consul2 - consul3 networks: - byfn 命令行参数-server：表示当前使用的 server 模式；如果没有指定，则表示是 client 模式。 -node：指定当前节点在集群中的名称。 -config-dir：指定配置文件路径，定义服务的；路径下面的所有 .json 结尾的文件都被访问；缺省值为：/consul/config。 -data-dir： consul 存储数据的目录；缺省值为：/consul/data。 -datacenter：数据中心名称，缺省值为 dc1。 -ui：使用 consul 自带的 web UI 界面 。 -join：加入到已有的集群中。 -retry-join：与 -join 类似，但允许重试连接，直到连接成功。一旦成功加入成员列表中的成员，它将永远不会尝试再次加入。然后，代理商将仅通过八卦维持其会员资格。允许配置多个 -retry-join，然后节点会按照顺序加入和重试，直到第一个成功。 -enable-script-checks： 检查服务是否处于活动状态，类似开启心跳。 -advertise：通告地址用于更改我们向集群中其他节点通告的地址。相当于 -bind 不可用时更改 -bind 地址，可以理解为 -bind 的一个备选方案。 -bind： 绑定服务器的 ip 地址，缺省值：“0.0.0.0”，这意味着 consul 将绑定到本地机器上的所有地址，并将私有 IPv4 地址通告给集群的其余节点，如果有多个私有 IPv4 地址可用，consul 将在启动时退出并报错。consul 1.1.0 之后，可以结合 go-sockaddr template使用，例如 1$ consul agent -bind &#39;&#123;&#123; GetPrivateInterfaces | include &quot;network&quot; &quot;10.0.0.0&#x2F;8&quot; | attr &quot;address&quot; &#125;&#125;&#39; -client：客户端可访问 ip，缺省值为：“127.0.0.1”，即仅允许环回连接。 -bootstrap-expect：在一个 datacenter 中期望的 server 节点数目，consul 启动时会一直等待直到达到这个数目的server才会引导整个集群。这个参数的值在同一个 datacenter 的所有server节点上必须保持一致。 -bootstrap：用来控制一个 server 是否运行在 bootstrap 模式：当一个 server 处于 bootstrap 模式时，它可以选举自己为 leader；注意在一个 datacenter 中只能有一个 server 处于 bootstrap 模式。所以这个参数一般只能用在只有一个 server 的开发环境中，在有多个 server 的 cluster 产品环境中，不能使用这个参数，否则如果多个 server 都标记自己为 leader 那么会导致数据不一致。另外该标记不能和 -bootstrap-expect 同时指定。 服务注册1、通过 config-file 注册服务，编辑 xxx.json，然后放在 /consul/config 目录下（默认情况）,当然也可以通过 consul 命令行加载配置。 12345678910111213141516171819202122&#123; \"services\": [ &#123; \"id\": \"hertz-demo-001\", \"name\": \"hertz-demo\", \"tags\": [ ], \"address\": \"192.168.0.103\", \"port\": 5000, \"checks\": [ &#123; \"http\": \"http://192.168.0.103:5000/ping\", \"tlsSkipVerify\": false, \"method\": \"GET\", \"interval\": \"10s\", \"timeout\": \"1s\", \"deregisterCriticalServiceAfter\": \"30s\" &#125; ] &#125; ] &#125; 2、执行 consul 命令重载配置文件 1consul reload 3、启动 web 服务，这里使用自己熟悉的语言写一个简单的服务即可，这里用的是 go http 框架 hertz: 1234567891011121314151617181920212223package mainimport ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" \"log\")func main() &#123; s := server.Default( server.WithHostPorts(\"192.168.0.103:5000\"), ) s.GET(\"/ping\", func(ctx context.Context, req *app.RequestContext) &#123; log.Print(req.ClientIP(), \" ping\") req.JSON(consts.StatusOK, utils.H&#123;\"ping\": \"pong\"&#125;) &#125;) s.Spin()&#125; 4、查看服务健康信息，passing 参数表示是否过滤不健康的服务 1curl http://127.0.0.1:8500/v1/health/service/hertz-demo\\?passing\\=true 5、注销服务 1curl --request PUT 127.0.0.1:8501/v1/agent/service/deregister/hertz-demo-001 6、consul ui 界面上显示如下： 注意事项1、注册服务时，check 参数 method GET 注意 GET 需要大写，否则健康检查会失败 2、服务注册的 client agent 挂了，那么 consul 会认为服务也挂了，并不会做故障转移，也不会同步原本 client agent 下的服务信息 3、服务注册需要确保网络能联通 扩展1、官方推荐的架构方式 官方建议一个集群部署 3-5 个 server agent，每个服务的服务器部署一个 client agent，如下如所示： 2、如是想要统一管理 consul agent，那可以参考另一种架构方式： 参考：https://mp.weixin.qq.com/s/ecmqqWuMho2a0xhaF1vFNAhttps://www.cnblogs.com/brady-wang/p/14440649.html","categories":[],"tags":[{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-03-25T16:04:15.447Z","updated":"2023-03-25T16:04:15.447Z","comments":true,"path":"2023/03/26/hello-world/","link":"","permalink":"http://example.com/2023/03/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"mysql 单机多实例","slug":"mysql-单机多实例","date":"2022-12-16T09:26:58.000Z","updated":"2024-06-27T09:41:04.715Z","comments":true,"path":"2022/12/16/mysql-单机多实例/","link":"","permalink":"http://example.com/2022/12/16/mysql-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"为每个实例创建单独的配置文件及数据目录 my.cnf 配置文件如下 123456789[mysql]socket = /usr/local/mysql/instance/3306/data/mysql.sock[mysqld]basedir = /usr/local/mysqldatadir = /usr/local/mysql/instance/3306/dataport = 3306socket = /usr/local/mysql/instance/3306/data/mysql.socklog_error = /usr/local/mysql/instance/3306/data/mysql-errorpid_file = /usr/local/mysql/instance/3306/data/mysql.pid 初始化 data 目录 12345678910mysqld --initialize --basedir=/usr/local/mysql --datadir=/usr/local/mysql/instance/3308/data2022-12-14T15:14:26.003364Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2022-12-14T15:14:26.005452Z 0 [Warning] Setting lower_case_table_names=2 because file system for /usr/local/mysql/instance/3308/data/ is case insensitive2022-12-14T15:14:26.129712Z 0 [Warning] InnoDB: New log files created, LSN=457902022-12-14T15:14:26.151110Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2022-12-14T15:14:26.218698Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: ff66d330-7bc1-11ed-9e9b-119ad045e8ba.2022-12-14T15:14:26.249279Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.2022-12-14T15:14:26.884182Z 0 [Warning] CA certificate ca.pem is self signed.2022-12-14T15:14:26.977637Z 1 [Note] A temporary password is generated for root@localhost: phbSZ0nOis=e 启动 mysql 服务端 1mysqld --defaults-file=/usr/local/mysql/instance/3308/my.cnf &amp; mysql 客户端中更改用户密码，初始密码在初始化时能获取（忘记了可以在网上搜搜跳过认证流程） 1mysql -u root -p -S /usr/local/mysql/instance/3308/data/mysql.sock 将启动命令存到 123456789alias mysqld3306=\"mysqld --defaults-file=/usr/local/mysql/instance/3306/my.cnf &amp;\"alias mysqld3307=\"mysqld --defaults-file=/usr/local/mysql/instance/3307/my.cnf &amp;\"alias mysqld3308=\"mysqld --defaults-file=/usr/local/mysql/instance/3308/my.cnf &amp;\"alias mysql3306=\"mysql -u root -p -S /usr/local/mysql/instance/3306/data/mysql.sock\"alias mysql3307=\"mysql -u root -p -S /usr/local/mysql/instance/3307/data/mysql.sock\"alias mysql3308=\"mysql -u root -p -S /usr/local/mysql/instance/3308/data/mysql.sock\"alias mysqlkill3306=\"mysqladmin -uroot -p -S /usr/local/mysql/instance/3306/data/mysql.sock shutdown\"alias mysqlkill3307=\"mysqladmin -uroot -p -S /usr/local/mysql/instance/3307/data/mysql.sock shutdown\"alias mysqlkill3308=\"mysqladmin -uroot -p -S /usr/local/mysql/instance/3308/data/mysql.sock shutdown\"","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]}],"categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"ES","slug":"ES","permalink":"http://example.com/tags/ES/"},{"name":"分布式事务","slug":"分布式事务","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"jvm","slug":"jvm","permalink":"http://example.com/tags/jvm/"},{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"},{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]}