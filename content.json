{"meta":{"title":"Jerry Chan","subtitle":"","description":"","author":"Jerry Chan","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2023-03-25T16:16:19.000Z","updated":"2023-03-25T16:17:15.844Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-03-25T16:15:37.000Z","updated":"2023-03-25T16:16:54.923Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka 消费者高 cpu 问题排查","slug":"kafka-消费者高-cpu-问题排查","date":"2023-04-14T15:31:22.000Z","updated":"2023-04-14T15:48:38.289Z","comments":true,"path":"2023/04/14/kafka-消费者高-cpu-问题排查/","link":"","permalink":"http://example.com/2023/04/14/kafka-%E6%B6%88%E8%B4%B9%E8%80%85%E9%AB%98-cpu-%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","excerpt":"","text":"今天本来打算愉快的划水，运维小哥突然找我说测试环境应用 cpu 一直居高不下，我一看告警还真是… cpu 高问题排查思路首先还是老套路： 先查看 cpu 高的线程是哪些 1top -Hp &lt;pid&gt; 查看线程的堆栈信息 12345//将线程 id 转换为 16 进制printf '%x\\n' &lt;tid&gt;//获取线程号后 50 行堆栈信息jstack &lt;pid&gt; | grep &lt;tid&gt; -A 50 这里我就直接用线程名称去查了。 看堆栈信息定位到是 kafka 消费者消费消息，导致 cpu 居高不下，正常情况下 kafka 消费者 cpu 飚高都是有大量的消息，我第一个感觉就是测试在进行压测，结果一看，打脸了🤔。 可以看到，lag 是 0 或者负数，我又刷新了几次基本上没有多少消息，这里留个心眼，后面我们在好好唠唠。 那么问题就来了，没有消息为啥消费者 cpu 会飚高… 突然灵机一动，会不会是消费者数太多了，导致循环去调用 poll 方法，造成整个节点 cpu 飚高。然后就屁颠屁颠的把所有主题的消费者数调小了，然鹅想想很美好，现实很骨感… 重启后节点的 cpu 依然居高不下。 遇到不懂就问度娘在网上逛了一圈，看到许多相似的场景，各种操作都试了一遍，还是没什么用🙃，kafka 的 github 仓库我也去逛了一圈，发现也有很多 cpu 高得场景，大多数都是建议升级客户端版本，忘了说我司目前用的还是 0.11.2，总之逛了一圈没有起到太大的帮助。 接着就是一波虾皮操作，显示生成了火焰图，看看 consumer poll 到底为啥一直占用 cpu，下面是一张火焰图： 虽然 poll 方法占用 cpu 耗时很长，但是仔细看又觉得没啥问题，是正常的在处理网络请求，这个时候我甚至一度怀疑是 kafka 客户端的 bug 😂。 有耐心问题迟早能解决还记的我们之前提了一嘴那张截图吗，没错就是下面这张 图中 lag 出现负数，其实 lag 出现负数还是很常见的，但问题就出在我排查了这么久图中的 lag 好像没变化过，而且一直是负数，那就很值得注意了。 我们还是先说说 lag 是怎么计算的 1lag = HW - consumerOffset HW: 高水位，通常等于所有 ISR 中最小的 LEO，详细的可以看看大佬的博客 consumerOffset: 表示消费者提交的消费位移 那么 lag 为啥会出现负数呢，由于我本身并未看过源码，所以从网上找了一个我认为比较是能说的通的解释: Producer 的 offset 是通过 JMX 轮询获得的，Consumer 的 offset 是从 kafka 内的 __consumer_offsets 的 topic 中直接读取到的，很明显轮询获取 offset 比 直接从 topic 拿 offset 慢一点，也就可能会出现 Lag 计算后为负数的情况。 OK，回到正题，lag 长时间是负数说明 consumerOffset 一直大于 HW，那么出现这个问题的原因大概率是 HW 一直不更新，因为 HW 只要更新其实 lag 很快就能变回 0。那么 HW 是什么时候更新的呢，其实是 Follower 副本同步 Leader 副本数据时，Leader 副本会对比 Follower 拉取数据的 offset 和 Leader 自身的 LEO 去更新 HW，所以通常 HW 需要 Follower 多同步一轮才会更新。 那么 HW 不更新，只能说明 Follower 没有去同步数据，想到这，我立马去看了下消费组的副本状态，发现有一个 broker 所有的分区副本都不在 ISR 中。那么基本上确定这个 broker 是出现问题了，但是这和我消费者 cpu 高有什么关系呢？ 这时运维小哥告诉我，poll 平均每 3ms 就请求一次，导致 cpu 飚高。纳尼？？？我的 poll timeout 明明是 100ms，怎么 3ms 一次呢，这明显有问题呀，运维小哥发了一下抓包的截图给我： kafka broker response 中提示 Not Leader For Partition，这不就和上面的猜想对上了吗，看看 chatgpt 给出的解释: 至此问题就排查了差不多了，那么接下来就是解决。由于是在测试环境发现的，解决方式也很粗暴，就是直接把 topic 直接删除了，然后重新创建。 小结 遇到涉及网络相关的问题，可以抓个包瞧瞧，说不定思路一下就打开了。 如果实在生产环境遇到这类问题，那么该怎么处理 broker 呢，这个得好好琢磨琢磨，目前思路是从 controller 下手。","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"wireshark 抓包 java应用中的 https 请求","slug":"wireshark-抓包-java应用中的-https-请求","date":"2023-04-03T07:08:41.000Z","updated":"2023-04-03T07:12:39.504Z","comments":true,"path":"2023/04/03/wireshark-抓包-java应用中的-https-请求/","link":"","permalink":"http://example.com/2023/04/03/wireshark-%E6%8A%93%E5%8C%85-java%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84-https-%E8%AF%B7%E6%B1%82/","excerpt":"","text":"最近生产中请求第三方接口的服务频繁告警，接口响应过慢，我们这边使用的 httpclient 连接池，各项配置看起来都没太大的问题，于是就想着说抓包看看是否有网络层面的问题还是的确是第三方接口慢。 通过 jsslkeylog 获取 sslkeylogwireshark 中 https 显示密文的样子有用过 wireshark 抓包 https 的大佬应该都知道，https 是有加密的，直接用 wireshark 抓包展示的全都是密文，如下图：可以看到，具体的 https 请求数据都被加密了。 如何让 https 在 wireshark 显示明文wireshark 中解密 https 请求的方式有多种，这里使用的方式是获取 https 请求时的 sslkeylog，使用到了一个 javaagent 工具 jsslkeylog，通过修改字节码达到发送 https 请求时将使用到的 sslkeylog 写入到本地磁盘的效果。具体流程也很简单：1、下载 jsslkeylog jar 包2、启动命令中加入 -javaagent:/path_to_jar/jSSLKeyLog.jar=/path_to_log/sslkeylog.log3、启动应用发起 https 请求4、之后应该就会在配置的 /path_to_log 中看到对应的 sslkeylog5、之后将 log 配置到 wireshark 中，prfferences -&gt; protocols -&gt; tls配置完成之后，就能看到原本的密文已经变成明文了，之后就能愉快的分析了🌝","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"k8s 搭建","slug":"k8s-搭建","date":"2023-03-31T15:45:37.000Z","updated":"2023-03-31T15:46:40.276Z","comments":true,"path":"2023/03/31/k8s-搭建/","link":"","permalink":"http://example.com/2023/03/31/k8s-%E6%90%AD%E5%BB%BA/","excerpt":"","text":"k8s搭建说明本文系搭建 kubernetes v1.21.3 版本集群笔记，使用三台虚拟机作为 CentOS7.9 系统测试机，安装 kubeadm、kubelet、kubectl 均使用 yum 安装，网络组件选用的是 flannel。 环境准备部署集群没有特殊说明均使用 root 用户执行命令。 2.1 硬件信息 ip hostname mem disk explain 192.168.85.2 192.168.85.2 2G 40GB k8s 控制平面节点 192.168.85.3 192.168.85.3 2G 40GB k8s 执行节点1 192.168.85.4 192.168.85.4 2G 40GB k8s 执行节点2 2.2 软件信息 12345software versionCentOS CentOS Linux release 7.9.2009 (Core)Kubernetes 1.21.3Docker 20.10.8Kernel 5.4.138-1.el7.elrepo.x86_64 2.3 禁用 swapswap 仅当内存不够时会使用硬盘块充当额外内存，硬盘的 io 较内存差距极大，禁用 swap 以提高性能各节点均需执行： 123swapoff -a cp /etc/fstab /etc/fstab.bakcat /etc/fstab.bak | grep -v swap &gt; /etc/fstab 2.4 关闭 SELinux 关闭 SELinux，否则 kubelet 挂载目录时可能报错 Permission denied，可以设置为 permissive 或 disabled，permissive 会提示 warn 信息各节点均需执行： 12setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 2.5 设置时区、同步时间 12timedatectl set-timezone Asia/Shanghai systemctl enable --now chronyd 查看同步状态： 1timedatectl status 2.6 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld 安装必要依赖1yum install -y yum-utils device-mapper-persistent-data lvm2 3.1 添加 aliyun docker-ce yum 源 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 重建 yum 缓存 1yum makecache fast 3.2 安装 Docker 查看可用 docker 版本 1yum list docker-ce.x86_64 --showduplicates | sort -r ==安装指定版本 Docker== yum install -y docker-ce-20.10.14-3.el7 这里以安装 20.10.14 版本举例，注意版本号不包含 : 与之前的数字。 3.3 确保网络模块开机自动加载 12lsmod | grep overlay lsmod | grep br_netfilter 若上面命令无返回值输出或提示文件不存在，需执行以下命令： 123456cat &gt; /etc/modules-load.d/docker.conf &lt;&lt;EOF overlay br_netfilter EOFmodprobe overlay modprobe br_netfilter 3.4 使桥接流量对 iptables 可见各个节点均需执行： 12345cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 验证是否生效，均返回 1 即正确。 12sysctl -n net.bridge.bridge-nf-call-iptables sysctl -n net.bridge.bridge-nf-call-ip6tables 3.5 配置 Docker 1mkdir /etc/docker 修改 cgroup 驱动为 systemd [k8s官方推荐]、限制容器日志量、修改存储类型，最后的 docker 家目录可修改： 123456789101112131415cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"registry-mirrors\": [\"https://gp8745ui.mirror.aliyuncs.com\"], \"data-root\": \"/data/docker\"&#125;EOF 服务脚本第 13 行修改： 123vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --default-ulimit core=0:0systemctl daemon-reload 添加开机自启，立即启动： 1systemctl enable --now docker 3.6 验证 Docker 是否正常 查看docker信息，判断是否与配置一致 部署 Kubernetes 集群如未说明，各节点均需执行如下步骤： 4.1 添加 kubernetes 源 123456789cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 重建yum缓存，输入y添加证书认证 1yum makecache fast 4.2 安装 kubeadm、kubelet、kubectl各节点均需安装 kubeadm、kubelet 12345yum list docker-ce.x86_64 --showduplicates | sort -rversion=1.21.3-0yum install -y kubelet-v1.21.0 kubeadm-v1.21.0 kubectl-v1.21.0systemctl enable kubelet --now 4.3 配置自动补全命令 安装 bash 自动补全插件 1yum install bash-completion -y 设置 kubectl 与 kubeadm 命令补全，下次 login 生效 12kubectl completion bash &gt;/etc/bash_completion.d/kubectlkubeadm completion bash &gt; /etc/bash_completion.d/kubeadm 4.4 为 Docker 设定使用的代理服务(暂跳过该步骤，由阿里云镜像解决)Kubeadm 部署 Kubernetes 集群的过程中，默认使用 Google 的 Registry 服务 k8s.gcr.io 上的镜像，例如k8s.grc.io/kube-apiserver 等，但国内无法访问到该服务。必要时，可自行设置合适的代理来获取相关镜像，或者从 Dockerhub 上下载镜像至本地后自行对镜像打标签。 这里简单说明一下设置代理服务的方法。编辑 /lib/systemd/system/docker.service 文件，在 [Service] 配置段中添加类似如下内容，其中的 PROXY_SERVER_IP 和 PROXY_PORT 要按照实际情况修改。 123Environment=\"HTTP_PROXY=http://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"HTTPS_PROXY=https://$PROXY_SERVER_IP:$PROXY_PORT\"Environment=\"NO_PROXY=192.168.4.0/24\" 配置完成后需要重载 systemd，并重新启动 docker 服务： 12systemctl daemon-reloadsystemctl restart docker.service 需要特别说明的是，由 kubeadm 部署的 Kubernetes 集群上，集群核心组件 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd 等均会以静态 Pod 的形式运行，它们所依赖的镜像文件默认来自于 k8s.gcr.io 这一 Registry 服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种，本示例将选择使用更易于使用的前一种方式： 使用能够到达该服务的代理服务使用国内的镜像服务器上的服务，例如 gcr.azk8s.cn/google_containers 和 registry.aliyuncs.com/google_containers 等（经测试，v1.22.0 版本已停用）4.5 查看指定 k8s 版本需要哪些镜像 123456789kubeadm config images list --kubernetes-version v1.21.0k8s.gcr.io/kube-apiserver:v1.21.0k8s.gcr.io/kube-controller-manager:v1.21.0k8s.gcr.io/kube-scheduler:v1.21.0k8s.gcr.io/kube-proxy:v1.21.0k8s.gcr.io/pause:3.4.1k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.0 4.6 拉取镜像 1vim pullimages.sh 1234567891011121314151617181920212223#!/bin/bash# pull images ver=v1.21.0registry=registry.cn-hangzhou.aliyuncs.com/google_containersimages=`kubeadm config images list --kubernetes-version=$ver |awk -F '/' '&#123;print $2&#125;'` for image in $imagesdoif [ $image != coredns ];then docker pull $&#123;registry&#125;/$image if [ $? -eq 0 ];then docker tag $&#123;registry&#125;/$image k8s.gcr.io/$image docker rmi $&#123;registry&#125;/$image else echo \"ERROR: 下载镜像报错，$image\" fielse docker pull coredns/coredns:1.8.0 docker tag coredns/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0 docker rmi coredns/coredns:1.8.0fidone 4.7 修改 kubelet 配置默认 cgroup driver 123456mkdir /var/lib/kubeletcat &gt; /var/lib/kubelet/config.yaml &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemdEOF 4.8 初始化 master 节点仅 192.168.85.2 节点需要执行此步骤。 4.8.1 生成 kubeadm 初始化配置文件[可选] 仅当需自定义初始化配置时用。 1kubeadm config print init-defaults &gt; kubeadm-config.yaml 修改配置文件： 12localAPIEndpoint: advertiseAddress: 1.2.3.4 替换为： 1234localAPIEndpoint: advertiseAddress: 192.168.85.2 name: 192.168.85.2kubernetesVersion: 1.21.0 123networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 替换为： 1234kubernetesVersion: 1.21.0networking: podSubnet: \"10.244.0.0/16\" serviceSubnet: 10.96.0.0/12 4.8.2 测试环境是否正常 1kubeadm init phase preflight 4.8.3 初始化 master10.244.0.0/16 是 flannel 固定使用的 IP 段，设置取决于网络组件要求。 1kubeadm init --config=kubeadm-config.yaml --ignore-preflight-errors=2 --upload-certs | tee kubeadm-init.log 4.8.4 为日常使用集群的用户添加 kubectl 使用权限 123456su - iuskyemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/admin.confsudo chown $(id -u):$(id -g) $HOME/.kube/admin.confecho \"export KUBECONFIG=$HOME/.kube/admin.conf\" &gt;&gt; ~/.bashrcexit 4.8.5 配置 master 认证 12echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' &gt;&gt; /etc/profile . /etc/profile 如果不配置这个，会提示如下输出：The connection to the server localhost:8080 was refused - did you specify the right host or port?此时 master 节点已经初始化成功，但是还未安装网络组件，还无法与其他节点通讯。 4.8.6 安装网络组件以 flannel 为例： 1234curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull quay.io/coreos/flannel:v0.14.0kubectl apply -f kube-flannel.yml 4.8.7 查看 192.168.85.2 节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 如果 STATUS 提示 NotReady，可以通过 kubectl describe node 192.168.85.2 查看具体的描述信息，性能差的服务器到达 Ready 状态时间会长些。 4.9 初始化 node 节点并加入集群4.9.1 获取加入 kubernetes 的命令访问 192.168.85.2 输入创建新 token 命令： 1kubeadm token create --print-join-command 同时输出加入集群的命令： 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 这个 token 也可以使用上述 master 上执行的初始化输出结果。 4.9.2 在 node 节点上执行加入集群的命令 1kubeadm join 192.168.85.2:6443 --token zukr14.dg1pxt9k9gndzqkl --discovery-token-ca-cert-hash sha256:0b57947ccd86cea8b7af2490fde858f3870e63bf35bbb0a567c702029376e9e5 4.10 查看集群节点状态 123456kubectl get nodesNAME STATUS ROLES AGE VERSION192.168.85.2 Ready control-plane,master 15d v1.21.0192.168.85.3 Ready &lt;none&gt; 15d v1.21.0192.168.85.4 Ready &lt;none&gt; 15d v1.21.0 4.11 部署 Dashboard4.11.1 部署 1curl -o recommended.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 默认 Dashboard 只能集群内部访问，修改 Service 为 NodePort 类型，暴露到外部： 12345678910111213141516vi recommended.yamlkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: ports: - port: 443 targetPort: 8443 nodePort: 30001 type: NodePort selector: k8s-app: kubernetes-dashboard 12345678910111213kubectl apply -f recommended.yaml # 这里下载镜像非常慢，我还是先手动拉下来吧，不行就多试几次docker pull kubernetesui/dashboard:v2.3.1docker pull kubernetesui/metrics-scraper:v1.0.6kubectl apply -f recommended.yamlkubectl get pods,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 0/1 ContainerCreating 0 52spod/kubernetes-dashboard-67484c44f6-shtz7 0/1 ContainerCreating 0 52sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 52sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 53s 查看状态正在创建容器中，稍后再次查看： 123456NAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-856586f554-nb68k 1/1 Running 0 2m11spod/kubernetes-dashboard-67484c44f6-shtz7 1/1 Running 0 2m11sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.188.208 &lt;none&gt; 8000/TCP 2m11sservice/kubernetes-dashboard NodePort 10.97.164.152 &lt;none&gt; 443:30001/TCP 2m12s 访问地址：https://NodeIP:30001；使用 Firefox 浏览器，Chrome 浏览器打不开不信任 SSL 证书的网站。 创建 service account 并绑定默认 cluster-admin 管理员集群角色： 123kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/&#123;print $1&#125;') 这里需要注意粘贴的时候有可能被换行，如果被换行，可在记事本中设置为一行。 使用输出的 token 登录 Dashboard。 问题解决1、dial tcp 10.96.0.1:443: connect: no route to host 123456systemctl stop dockersystemctl stop kubeletiptables --flushiptables -tnat --flushsystemctl start kubeletsystemctl start docker 2、failed to delegate add: failed to set bridge addr: “cni0“ already has an IP address different from 1 1https://blog.csdn.net/weixin_42562106/article/details/123749291 3、failed to add vxlanRoute (10.244.0.0/24 -&gt; 10.244.0.0): network is down 1234ip link delete flannel.1systemctl restart networkkubectl delete -f kube-flannel.ymlkubectl apply -f kube-flannel.yml 4、Get http://10.244.0.3:8181/ready: dial tcp 10.244.0.3:8181: connect: connection refused 12#重新corednskubectl -n kube-system rollout restart deployment/coredns 以上几个问题遇到可以先尝试重启所有的机器，如果不行在通过上述方案解决 参考：https://www.iuskye.com/2021/08/10/k8s-kubeadm-1213.html","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"redisson3.15.2 公平锁任务丢失","slug":"redisson3.15.2 公平锁任务丢失","date":"2023-03-31T15:22:45.000Z","updated":"2023-03-31T15:37:33.829Z","comments":true,"path":"2023/03/31/redisson3.15.2 公平锁任务丢失/","link":"","permalink":"http://example.com/2023/03/31/redisson3.15.2%20%E5%85%AC%E5%B9%B3%E9%94%81%E4%BB%BB%E5%8A%A1%E4%B8%A2%E5%A4%B1/","excerpt":"","text":"场景： 线索流转改成 redisson 公平锁，任务耗时长，导致部分任务丢失。 一、redisson 公平锁实现1、redis 中的 K/V12345678list: redisson_lock_queue:&#123;test&#125; elem: UUID:threadIdzset: redisson_lock_timeout:&#123;key&#125; elem: UUID:threadId score: timeout = ttl + 线程等待时间(5*60000ms) + 当前时间戳hset: key hashKey: UUID:threadId hashVal: 1 2、上锁流程（这里就不放源码了，感兴趣可以自己看看） org.redisson.RedissonFairLock#tryLockInnerAsync 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 如果当前没有线程占用锁，则上锁 (ttl = watchdogtimeout)，同时redisson_lock_timeout:{key} 中所有的 score - 线程等待时间(5*60000ms) 如果存在锁，且是被当前线程占用的，则 hashVal 加一 如果存在锁，且不是被当前线程占用的，同时已经加入过等待队列，则返回当前线程在队列中的 ttl 如果存在锁，且不是被当前线程占用的，并且未加入等待队列，则加入等待队列，timeout= ttl + 线程等待时间(5*60000ms) + 当前时间戳，ttl 为队列中最后一个元素的 timeout - current 或者 锁的超时时间 3、订阅 redisson_lock__channel:{fairLock}:UUID:threadId此处阻塞线程，直到消息队列中有消息发送为止 4、解锁流程 清除 redisson_lock_timeout:{key} 中 score 小于当前时间戳的elem，同时清除对应 redisson_lock_queue:{test} 中的 elem 判断锁是否存在，如果不存在则说明锁已经被释放了，判断等待队列中是否有elem，有的话取出第一个 publish message 如果锁存在，且非本线程持有，则直接返回 null 如果所存在，且是当前线程持有，并且获取所次数大于 1，则 hashVal减一，更新锁超时时间 如果所存在，且是当前线程持有，释放锁，判断等待队列中是否有elem，有的话取出第一个 publish message 5、获取到锁之后，取消订阅 redisson_lock__channel:{fairLock}:UUID:threadId 二、问题分析1、由于测试环境第三方鉴权接口较慢，每分配一条线索需要3-4s 2、存量线索有 1500+ 条，每 200 条作为一个任务，总共拆分 8 个任务，每个任务需要执行 200*3 = 600+ s 3、redisson 公平锁线程等待时间 (5*60000ms)，也就是说单个任务执行完至少会有一个任务过期，在 unlock 或者 lock 操作是会先清除过期任务 4、由于获取不到锁，线程会订阅 redisson_lock__channel:{fairLock}:UUID:threadId，阻塞知道接收到消息，又由于等待队列里的线程被清了，这个消息队列永远不会收到消息，所以线程一直阻塞，且任务无法执行，资源被占用。 5、代码模拟： 1234567891011121314151617public static void main(String[] args) &#123; Config config = new Config(); config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\"); Redisson redissonClient = (Redisson) Redisson.create(config); IntStream.range(0, 10).forEach(i -&gt; &#123; new Thread(() -&gt; &#123; //RedissonFairLock lock = (RedissonFairLock) redissonClient.getFairLock(\"test\"); RedissonFairLock lock = new RedissonFairLock(redissonClient.getCommandExecutor(), \"test\", 2000); lock.lock(); //最终输出次数小于10 System.out.println(i); ThreadUtil.sleep(10000); lock.unlock(); &#125;).start(); &#125;); &#125; 参考: Redisson分布式锁之公平锁原理","categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"apollo 集群架构","slug":"apollo-集群架构","date":"2023-03-25T18:25:23.000Z","updated":"2023-03-31T15:44:48.277Z","comments":true,"path":"2023/03/26/apollo-集群架构/","link":"","permalink":"http://example.com/2023/03/26/apollo-%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/","excerpt":"","text":"apollo 集群架构apollo 组成结构apollo 集群主要有三个部分组成，config-service，admin-service，portal config-service: 主要为应用客户端提供服务，包括配置的读取、推送等功能，config-service 内置 eureka，已提供 admin-service, portal 的服务发现、服务注册 admin-service: 主要为 apollo-portal 提供服务，包括应用配置管理、发布等功能 portal: 是 apollo 提供的服务配置前端页面 apollo 部署方案这个官网提供的高可用双环境架构，更多的架构方案可参考 docker-compose 部署 apollo 集群 执行 sql 脚本，生成 apollo 需要的 db 表 编写 docker-compose 文件，本次只搭建开发环境模拟单机单环境，config-service，admin-service，portal 都各自部署一个容器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: '2'networks: net:services: #开发环境configService，eureka apollo-configservice-dev: image: apolloconfig/apollo-configservice container_name: configservice-dev ports: - 8080:8080 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - eureka.instance.ip-address=192.168.0.103 networks: - net apollo-adminservice-dev: image: apolloconfig/apollo-adminservice container_name: adminservice-dev ports: - 8090:8090 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloConfigDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 #这里需要配置开环环境的configService - eureka.service.url=http://configservice-dev:8080/eureka/ depends_on: - apollo-configservice-dev networks: - net apollo-portal: image: apolloconfig/apollo-portal container_name: apollo-portal ports: - 8070:8070 environment: - SPRING_DATASOURCE_URL=jdbc:mysql://192.168.0.103:3306/ApolloPortalDB?characterEncoding=utf8 - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - spring.profiles.active=auth - APOLLO_PORTAL_ENVS=dev,fat,pre,gray,pro - DEV_META=http://configservice-dev:8080 # 以下这些暂时使用开发环境的 - FAT_META=http://configservice-dev:8080 - PRE_META=http://configservice-dev:8080 - GRAY_META=http://configservice-dev:8080 - PRO_META=http://configservice-dev:8080 depends_on: - apollo-adminservice-dev networks: - net","categories":[],"tags":[{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"consul 集群搭建及注意事项","slug":"consul-集群搭建及注意事项","date":"2023-03-25T17:16:50.000Z","updated":"2023-03-31T15:45:02.909Z","comments":true,"path":"2023/03/26/consul-集群搭建及注意事项/","link":"","permalink":"http://example.com/2023/03/26/consul-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","excerpt":"","text":"集群搭建本次利用 docker 搭建 consul 集群，利用 docker-compose 统一管理 集群包含三个 server agent: node1、node2、node3 集群包含两个 client agent: node4、node5，client agent 提供 ui **1、下载 docker 镜像 ** 1docker pull docker 2、编辑 docker-compose.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253version: '2'networks: byfn:services: consul1: image: consul container_name: node1 command: agent -server -bootstrap-expect=3 -node=node1 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 networks: - byfn consul2: image: consul container_name: node2 command: agent -server -retry-join=node1 -node=node2 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul3: image: consul container_name: node3 command: agent -server -retry-join=node1 -node=node3 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 depends_on: - consul1 networks: - byfn consul4: image: consul container_name: node4 command: agent -retry-join=node1 -node=ndoe4 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8500:8500 depends_on: - consul2 - consul3 networks: - byfn consul5: image: consul container_name: node5 command: agent -retry-join=node1 -node=ndoe5 -bind=0.0.0.0 -client=0.0.0.0 -datacenter=dc1 -ui ports: - 8501:8500 depends_on: - consul2 - consul3 networks: - byfn 命令行参数-server：表示当前使用的 server 模式；如果没有指定，则表示是 client 模式。 -node：指定当前节点在集群中的名称。 -config-dir：指定配置文件路径，定义服务的；路径下面的所有 .json 结尾的文件都被访问；缺省值为：/consul/config。 -data-dir： consul 存储数据的目录；缺省值为：/consul/data。 -datacenter：数据中心名称，缺省值为 dc1。 -ui：使用 consul 自带的 web UI 界面 。 -join：加入到已有的集群中。 -retry-join：与 -join 类似，但允许重试连接，直到连接成功。一旦成功加入成员列表中的成员，它将永远不会尝试再次加入。然后，代理商将仅通过八卦维持其会员资格。允许配置多个 -retry-join，然后节点会按照顺序加入和重试，直到第一个成功。 -enable-script-checks： 检查服务是否处于活动状态，类似开启心跳。 -advertise：通告地址用于更改我们向集群中其他节点通告的地址。相当于 -bind 不可用时更改 -bind 地址，可以理解为 -bind 的一个备选方案。 -bind： 绑定服务器的 ip 地址，缺省值：“0.0.0.0”，这意味着 consul 将绑定到本地机器上的所有地址，并将私有 IPv4 地址通告给集群的其余节点，如果有多个私有 IPv4 地址可用，consul 将在启动时退出并报错。consul 1.1.0 之后，可以结合 go-sockaddr template使用，例如 1$ consul agent -bind &#39;&#123;&#123; GetPrivateInterfaces | include &quot;network&quot; &quot;10.0.0.0&#x2F;8&quot; | attr &quot;address&quot; &#125;&#125;&#39; -client：客户端可访问 ip，缺省值为：“127.0.0.1”，即仅允许环回连接。 -bootstrap-expect：在一个 datacenter 中期望的 server 节点数目，consul 启动时会一直等待直到达到这个数目的server才会引导整个集群。这个参数的值在同一个 datacenter 的所有server节点上必须保持一致。 -bootstrap：用来控制一个 server 是否运行在 bootstrap 模式：当一个 server 处于 bootstrap 模式时，它可以选举自己为 leader；注意在一个 datacenter 中只能有一个 server 处于 bootstrap 模式。所以这个参数一般只能用在只有一个 server 的开发环境中，在有多个 server 的 cluster 产品环境中，不能使用这个参数，否则如果多个 server 都标记自己为 leader 那么会导致数据不一致。另外该标记不能和 -bootstrap-expect 同时指定。 服务注册1、通过 config-file 注册服务，编辑 xxx.json，然后放在 /consul/config 目录下（默认情况）,当然也可以通过 consul 命令行加载配置。 12345678910111213141516171819202122&#123; \"services\": [ &#123; \"id\": \"hertz-demo-001\", \"name\": \"hertz-demo\", \"tags\": [ ], \"address\": \"192.168.0.103\", \"port\": 5000, \"checks\": [ &#123; \"http\": \"http://192.168.0.103:5000/ping\", \"tlsSkipVerify\": false, \"method\": \"GET\", \"interval\": \"10s\", \"timeout\": \"1s\", \"deregisterCriticalServiceAfter\": \"30s\" &#125; ] &#125; ] &#125; 2、执行 consul 命令重载配置文件 1consul reload 3、启动 web 服务，这里使用自己熟悉的语言写一个简单的服务即可，这里用的是 go http 框架 hertz: 1234567891011121314151617181920212223package mainimport ( \"context\" \"github.com/cloudwego/hertz/pkg/app\" \"github.com/cloudwego/hertz/pkg/app/server\" \"github.com/cloudwego/hertz/pkg/common/utils\" \"github.com/cloudwego/hertz/pkg/protocol/consts\" \"log\")func main() &#123; s := server.Default( server.WithHostPorts(\"192.168.0.103:5000\"), ) s.GET(\"/ping\", func(ctx context.Context, req *app.RequestContext) &#123; log.Print(req.ClientIP(), \" ping\") req.JSON(consts.StatusOK, utils.H&#123;\"ping\": \"pong\"&#125;) &#125;) s.Spin()&#125; 4、查看服务健康信息，passing 参数表示是否过滤不健康的服务 1curl http://127.0.0.1:8500/v1/health/service/hertz-demo\\?passing\\=true 5、注销服务 1curl --request PUT 127.0.0.1:8501/v1/agent/service/deregister/hertz-demo-001 6、consul ui 界面上显示如下： 注意事项1、注册服务时，check 参数 method GET 注意 GET 需要大写，否则健康检查会失败 2、服务注册的 client agent 挂了，那么 consul 会认为服务也挂了，并不会做故障转移，也不会同步原本 client agent 下的服务信息 3、服务注册需要确保网络能联通 扩展1、官方推荐的架构方式 官方建议一个集群部署 3-5 个 server agent，每个服务的服务器部署一个 client agent，如下如所示： 2、如是想要统一管理 consul agent，那可以参考另一种架构方式： 参考：https://mp.weixin.qq.com/s/ecmqqWuMho2a0xhaF1vFNAhttps://www.cnblogs.com/brady-wang/p/14440649.html","categories":[],"tags":[{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-03-25T16:04:15.447Z","updated":"2023-03-25T16:04:15.447Z","comments":true,"path":"2023/03/26/hello-world/","link":"","permalink":"http://example.com/2023/03/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"redisson","slug":"redisson","permalink":"http://example.com/categories/redisson/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"运维","slug":"运维","permalink":"http://example.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"redisson","slug":"redisson","permalink":"http://example.com/tags/redisson/"},{"name":"框架","slug":"框架","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6/"},{"name":"apollo","slug":"apollo","permalink":"http://example.com/tags/apollo/"},{"name":"consul","slug":"consul","permalink":"http://example.com/tags/consul/"}]}